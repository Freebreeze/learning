% Created 2016-10-11 Tue 17:39
\documentclass{beamer}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{etex}
\usepackage{amsmath}
\usepackage{pstricks}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage[europeanresistors,americaninductors]{circuitikz}
\usepackage{colortbl}
\usepackage{yfonts}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{intersections}
\usetikzlibrary{calc,patterns,decorations.pathmorphing,decorations.markings}
\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\setCJKmainfont[BoldFont=Evermore Hei]{Evermore Kai}
\setCJKmonofont{Evermore Kai}
\usepackage{pst-node}
\usepackage{pst-plot}
\psset{unit=5mm}
\mode<beamer>{\usetheme{Frankfurt}}
\mode<beamer>{\usecolortheme{dove}}
\mode<article>{\hypersetup{colorlinks=true,pdfborder={0 0 0}}}
\AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}
\setbeamercovered{transparent}
\providecommand{\alert}[1]{\textbf{#1}}

\title{增强学习}
\author{}
\date{}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.9.3f}}

\begin{document}

\maketitle

\begin{frame}
\frametitle{Outline}
\setcounter{tocdepth}{3}
\tableofcontents
\end{frame}














\section{简介}
\label{sec-1}
\begin{frame}
\frametitle{增强学习（Reinforcement Learning ）}
\label{sec-1-1}

\begin{itemize}
\item 增强学习要解决的问题：一个能够感知环境的自治agent，怎样学习选择能达到其目标的最优动作。
\begin{itemize}
\item 学习控制移动机器人、在工厂中学习进行最优操作工序、以及学习棋类对弈等。
\item 当agent在其环境中作出每个动作时，施教者会提供奖赏或惩罚信息，以表示结果状态的正确与否。
\begin{itemize}
\item 例如，在训练agent 进行棋类对弈时，施教者可在游戏胜利时给出正回报，而在游戏失败时给出负回报，其他时候为零回报。
\item Agent的任务就是从这个非直接的、有延迟的回报中学习，以便后续的动作产生最大的累积回报。
\end{itemize}
\end{itemize}
\item Q学习的算法：可从有延迟的回报中获取最优控制策略，即使agent 没有有关其动作会对环境产生怎样的效果的先验知识。
\item 增强学习与动态规划（dynamic programming）算法有关，后者常被用于解决最优化问题。
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{学习控制策略}
\label{sec-1-2}

\begin{itemize}
\item 学习控制策略以选择动作的问题在某种程度上类似于其他章讨论过的函数逼近问题。
\item 这里待学习的目标函数为控制策略 $\pi : S \rightarrow A$ 。它在给定当前状态 $S$ 集合中的 $s$ 时，从集合 $A$ 中输出一个合适的动作 $a$ 。
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{延迟回报（delayed reward）}
\label{sec-1-3}

然而，增强学习问题与其他的函数逼近问题有几个重要不同：
\begin{itemize}
\item 延迟回报（delayed reward）
\begin{itemize}
\item Agent的任务是学习一个目标函数 $\pi$ 。它把当前状态s映射到最优动作 $a=\pi (s)$  。
\item 在前面章节中，我们总是假定在学习 $\pi$ 这样的目标函数时，每个训练样例是序偶的形式  $\langle s,\pi (s)\rangle$ 。
\item 然而在增强学习中，训练信息不能以这种形式得到。
\item 相反，施教者只在agent执行其序列动作时提供一个序列立即回报值，因此agent面临一个时间信用分配（temporal credit assignment ）的问题：
     确定最终回报的生成应归功于其序列中哪一个动作。
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{探索（exploration ）}
\label{sec-1-4}

\begin{itemize}
\item 探索（exploration ）
\begin{itemize}
\item 在增强学习中，agent 通过其选择的动作序列影响训练样例的分布。
\item 这产生了一个问题：哪种实验策略可产生最有效的学习。学习器面临的是一个折中的问题：
\begin{itemize}
\item 是选择探索未知的状态和动作（以收集新信息），
\item 还是选择它已经学习过、会产生高回报的状态和动作（以使累积回报最大化）。
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{部分可观察状态（partially observable states）}
\label{sec-1-5}

\begin{itemize}
\item 部分可观察状态（partially observable states）
\begin{itemize}
\item 虽然为了方便起见，可以假定agent传感器在每一步可感知到环境的全部状态，但在实际的情况下传感器只能提供部分信息。
\item 例如：带有前向镜头的机器人不能看到它后面的情况。在此情况下可能需要结合考虑其以前的观察以及当前的传感器数据以选择动作，
         而最佳的策略有可能是选择特定的动作以改进环境可观察性。
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{长期学习（life-long learning）}
\label{sec-1-6}

\begin{itemize}
\item 长期学习（life-long learning）
\begin{itemize}
\item 不象分离的函数逼近任务，机器人学习问题经常要求此机器人在相同的环境下使用相同的传感器学习多个相关任务。
\item 怎样在窄小的走廊中行走，以及怎样从激光打印机中取得打印纸等。
\item 这使得有可能使用先前获得的经验或知识在学习新任务时减小样本复杂度。
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{示例}
\label{sec-1-7}

\begin{itemize}
\item 建造一个可学习机器人。
\begin{itemize}
\item 机器人（或agent）有一些传感器可以观察其环境的状态（state）并能做出一组动作（action）改变这些状态。
\item 移动机器人具有镜头和声纳等传感器，并可以做出“直走”和“转弯”等动作。
\end{itemize}
\item 学习的任务是获得一个控制策略（policy），以选择能达到目的的行为。
\begin{itemize}
\item 此机器人的任务是在其电池电量转低时找到充电器进行充电。
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{One Example: TD-Gammon}
\label{sec-1-8}


\begin{itemize}
\item Tesauro, 1995
\item Learn to play Backgammon
\item Immediate reward
\begin{itemize}
\item +100 if win
\item -100 if lose
\item 0 for all other states
\end{itemize}
\item Trained by playing 1.5 million games against itself
  Now approximately equal to best human player
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Reinforcement Learning Problem}
\label{sec-1-9}


\center
\includegraphics[width=0.5\textwidth]{./image/fig13-1.png}
\end{frame}
\section{学习任务}
\label{sec-2}
\begin{frame}
\frametitle{Markov Decision Processes}
\label{sec-2-1}


假设
\begin{itemize}
\item 有限状态集合 $S$
\item 行动集合 $A$
\item 在离散时间， agent 观测状态  $s_t \in S$ 选择行动 $a_t \in A$
\item 接受即时回报 $r_t$
\item 状态转换为 to $s_{t+1}$
\item Markov 假设:  $s_{t+1} = \delta(s_t, a_t)$ ,  $r_t = r(s_t,a_t)$
\begin{itemize}
\item $r_t$ 与 $s_{t+1}$ 只依赖当前状态与动作
\item 函数 $\delta$ 与 $r$ 可以是非确定的
\item 对agent来说，函数 $\delta$ 与 $r$ 可以是未知的
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Agent's Learning Task}
\label{sec-2-2}


在环境中执行动作,观察结果, 
\begin{itemize}
\item 学习动作策略 $\pi : S \rightarrow A$ ，   从 $S$ 中的任意初始状态最大化
    $$E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots ]$$
\item $0 \leq \gamma < 1$ 是未来回报的折算因子
\item 目标函数是  $\pi : S \rightarrow A$
\item 没有  $\langle s, a \rangle$ 形式的训练样例
\item 训练样例形式为 $\langle \langle  s, a \rangle , r \rangle$
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Value Function}
\label{sec-2-3}


考虑确定世界

对每个策略 $\pi$ ，定义评估函数

\begin{eqnarray}
 & V^{\pi}(s) & \equiv r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + ...
\nonumber \\ 
& & \equiv \sum_{i=0}^{\infty} \gamma^{i} r_{t+i} \nonumber
\end{eqnarray}

其中 $r_{t}, r_{t+1}, \ldots$ 按策略 $\pi$ 从状态 $s$ 开始生成。
任务是学习最优策略 $\pi^{*}$ $$\pi^{*} \equiv \arg\max_{\pi} V^{\pi}(s), (\forall s)$$
\end{frame}
\begin{frame}
\frametitle{A simple deterministic world to illustrate the basic concepts of $Q$ -learning.}
\label{sec-2-4}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{block}{$r(s,a)$ (immediate reward) values}
\label{sec-2-4-1}


\includegraphics[width=.9\linewidth]{./image/rl-grid-r.png}
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{block}{$Q(s,a)$ values}
\label{sec-2-4-2}


\includegraphics[width=.9\linewidth]{./image/rl-grid-q.png}
\end{block}
\end{column}
\end{columns}
\end{frame}
\begin{frame}
\frametitle{A simple deterministic world to illustrate the basic concepts of $Q$ -learning.}
\label{sec-2-5}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{block}{$V^{*}(s)$ values}
\label{sec-2-5-1}



\includegraphics[width=.9\linewidth]{./image/rl-grid-v.png}
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{block}{One optimal policy}
\label{sec-2-5-2}


\includegraphics[width=.9\linewidth]{./image/rl-grid-policy.png}
\end{block}
\end{column}
\end{columns}
\end{frame}
\begin{frame}
\frametitle{What to Learn}
\label{sec-2-6}

学习评估函数 $V^{\pi^{*}}$ (记作 $V^*$)


从任意状态 $s$ 前瞻性搜索选择最优行动

\[ \pi^{*}(s) = \arg\max_{a} [r(s,a) + \gamma V^{*}(\delta(s,a))] \]

问题：
\begin{itemize}
\item \item This works well if agent knows $\delta: S \times A \rightarrow S$, and $r : S \times A \rightarrow \Re$
\item \item But when it doesn't, it can't choose actions this way
\end{itemize}
\end{frame}
\section{$Q$ 学习}
\label{sec-3}
\begin{frame}
\frametitle{$Q$ Function}
\label{sec-3-1}


与 $V^*$ 类似定义新函数

\[ Q(s,a) \equiv r(s,a) + \gamma V^{*}(\delta(s,a)) \]

若 agent 学习 $Q$, 可以在不知道 $\delta$ 的情况下选取最优行动!

$$\pi^{*}(s) = \arg\max_{a} [r(s,a) + \gamma V^{*}(\delta(s,a))]$$

\[ \pi^{*}(s) = \arg\max_{a} Q(s,a) \]

$Q$ 是agent 将要学习的评估函数
\end{frame}
\begin{frame}
\frametitle{Training Rule to Learn $Q$}
\label{sec-3-2}


 $Q$ 与 $V^*$ 有关:
\[  V^{*}(s) = \max_{a'}Q(s,a') \]

 $Q$ 可以递归表示：

\begin{eqnarray}
Q(s_t,a_t) &= &  r(s_t,a_t) + \gamma V^{*}(\delta(s_t,a_t))) \nonumber \\
 &= &  r(s_t,a_t) + \gamma \max_{a'}Q(s_{t+1},a') \nonumber
\end{eqnarray}

设 $\hat{Q}$ 表示当前对 $Q$ 的逼近.  考虑训练规则

\[ \hat{Q}(s,a) \leftarrow r + \gamma \max_{a'}\hat{Q}(s',a') \]

其中 $s'$ 是在状态 $s$ 应用行动 $a$ 后得到的新状态
\end{frame}
\begin{frame}
\frametitle{$Q$ Learning for Deterministic Worlds}
\label{sec-3-3}


对每个 $s, a$ 初始化initialize table entry $\hat{Q}(s,a) \leftarrow 0$

\begin{itemize}
\item 对每个 $s,a$ ，初始化表项 $\hat{Q}(s,a) \leftarrow 0$
\item 观察当前状态 $s$
\item 一直重复：
\begin{itemize}
\item 选择一个动作 $a$ 并执行它
\item 接收到立即回报 $r$
\item 观察新状态 $s'$
\item 对 $\hat{Q}(s,a)$ 按照下式更新表项：
        \begin{displaymath}
        \hat{Q}(s,a) \leftarrow r + \gamma \max_{a'}\hat{Q}(s',a') 
        \end{displaymath}
\item $s \leftarrow s'$
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Updating $\hat{Q}$}
\label{sec-3-4}

\includegraphics[width=.9\linewidth]{./image/rl-grid-trace.png}
\end{frame}
\begin{frame}
\frametitle{Updating $\hat{Q}$}
\label{sec-3-5}

\begin{eqnarray}
\hat{Q}(s_1,a_{right}) & \leftarrow & r + \gamma \max_{a'}\hat{Q}(s_2,a') \nonumber \\
 & \leftarrow & 0 + 0.9 \ \max \{63, 81, 100 \} \nonumber \\ & \leftarrow & 90  \nonumber
\end{eqnarray}

若回报非负，则
\[(\forall s,a,n)\ \ \hat{Q}_{n+1}(s,a) \geq \hat{Q}_{n}(s,a)\]
\[(\forall s,a,n)\ \  0 \leq \hat{Q}_n(s,a) \leq Q(s,a)\]

$\hat{Q}$ 收敛到 $Q$.  考虑确定世界，每个 $\langle s,a \rangle$ 无限频繁访问。
\end{frame}
\begin{frame}
\frametitle{证明}
\label{sec-3-6}

 在一个完全区间（ full interval）（一个区间，其间每个 $\langle s, a \rangle$ 都被访问. ）， $\hat{Q}$ 表中的最大误差按因子 $\gamma$ 减小。
\end{frame}
\begin{frame}
\frametitle{证明(续)}
\label{sec-3-7}

 令 $\hat{Q}_{n}$ 为  $n$ 次更新后的表， $\Delta_{n}$ 是 $\hat{Q}_{n}$ 中的最大误差，即：
\[\Delta_{n} = \max_{s,a} |\hat{Q}_{n}(s,a) - Q(s,a)| \]
 对在第 $n+1$ 次更新的任意表项 $\hat{Q}_{n}(s,a)$, 在修正后的估计 $\hat{Q}_{n+1}(s,a)$ 中的误差为：
    \begin{eqnarray}
    |\hat{Q}_{n+1}(s,a) - Q(s,a)| & = & | (r + \gamma \max_{a'}\hat{Q}_{n}(s',a'))  - (r + \gamma \max_{a'}Q(s',a')) | \nonumber \\
     & = & \gamma | \max_{a'}\hat{Q}_{n}(s',a') - \max_{a'}Q(s',a') | \nonumber\\
     & \leq & \gamma \max_{a'} | \hat{Q}_{n}(s',a') - Q(s',a') | \nonumber \\
     & \leq & \gamma \max_{s'',a'} | \hat{Q}_{n}(s'',a') - Q(s'',a') | \nonumber \\
    |\hat{Q}_{n+1}(s,a) - Q(s,a)| & \leq & \gamma \Delta_{n} \nonumber
    \end{eqnarray}
 注：对任意两个函数，下式成立：
    \[|\max_{a}f_{1}(a) - \max_{a}f_{2}(a)| \leq \max_{a} |f_{1}(a)-f_{2}(a)|\]
\end{frame}
\section{非确定性回报和动作}
\label{sec-4}
\begin{frame}
\frametitle{Nondeterministic Case}
\label{sec-4-1}


回报与下一个状态是非确定性的

通过求期望重定义 $V, Q$ by

\begin{eqnarray}
 & V^{\pi}(s) & \equiv E[ r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + \ldots ] \nonumber \\ 
& & \equiv E [ \sum_{i=0}^{\infty} \gamma^{i} r_{t+i} ] \nonumber
\end{eqnarray}

\[Q(s,a) \equiv E[r(s,a) + \gamma V^{*}(\delta(s,a))]\]
\end{frame}
\begin{frame}
\frametitle{$Q$ learning generalizes to nondeterministic worlds}
\label{sec-4-2}


将 $Q$ learning 推广到非确定性世界

将训练规则改为
\[\hat{Q}_{n}(s,a)  \leftarrow  (1-\alpha_{n})\hat{Q}_{n-1}(s,a) + \alpha_{n}[r + \max_{a'}\hat{Q}_{n-1}(s',a')]\]
其中
\[\alpha_{n} = \frac{1}{1 + visits_n(s,a)}\]

仍可证明 $\hat{Q}$ 收敛至 $Q$ [Watkins and Dayan, 1992]
\end{frame}
\section{Temporal Difference Learning}
\label{sec-5}
\begin{frame}
\frametitle{Temporal Difference Learning}
\label{sec-5-1}

$Q$ learning: 减小相继 $Q$ 估计之间的不一致

单步时间差分:
\[Q^{(1)}(s_t,a_t) \equiv r_t + \gamma \max_{a} \hat{Q}(s_{t+1},a)\]

两步
\[Q^{(2)}(s_t,a_t) \equiv r_t + \gamma r_{t+1} + \gamma^2 \max_{a}\hat{Q}(s_{t+2},a) \]

 $n$ 步
\[ Q^{(n)}(s_t,a_t) \equiv r_t + \gamma r_{t+1} + \cdots + \gamma^{(n-1)}r_{t+n-1} + \gamma^n \max_{a}\hat{Q}(s_{t+n},a) \]

混合多步:
\[Q^{\lambda}(s_{t},a_{t})  \equiv (1- \lambda) \left[
Q^{(1)}(s_t,a_t) + \lambda Q^{(2)}(s_t,a_t) + \lambda^2 Q^{(3)}(s_t,a_t) +
\cdots \right] \]
\end{frame}
\begin{frame}
\frametitle{Temporal Difference Learning}
\label{sec-5-2}

\[Q^{\lambda}(s_{t},a_{t})  \equiv (1- \lambda) \left[
Q^{(1)}(s_t,a_t) + \lambda Q^{(2)}(s_t,a_t) + \lambda^2 Q^{(3)}(s_t,a_t) +
\cdots \right] \]

等效表达式:
\begin{eqnarray*}
 Q^{\lambda}(s_{t},a_{t}) & = r_{t} + \gamma [ & (1 -
\lambda)
\max_{a}\hat{Q}(s_{t},a_{t}) \\
 & & + \lambda \ Q^{\lambda}(s_{t+1},a_{t+1})]
\end{eqnarray*}


TD($\lambda$) 算法使用上述训练规则
\begin{itemize}
\item 有时收敛比 $Q$ learning 快
\item 学习 $V^*$ 对任意 $0 \leq \lambda \leq 1$ 收敛 (Dayan, 1992)
\item Tesauro's TD-Gammon uses this algorithm
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Subtleties and Ongoing Research}
\label{sec-5-3}


\begin{itemize}
\item Replace $\hat{Q}$ table with neural net or other generalizer
\item Handle case where state only partially observable
\item Design optimal exploration strategies
\item Extend to continuous action, state
\item Learn and use $\hat{\delta}: S \times A \rightarrow S$
\item Relationship to dynamic programming
\end{itemize}
\end{frame}

\end{document}
