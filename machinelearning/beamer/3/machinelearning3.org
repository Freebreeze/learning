 # +LaTeX_CLASS: article
#+LATEX_HEADER: \usepackage{etex}
#+LATEX_HEADER: \usepackage{amsmath}
 # +LATEX_HEADER: \usepackage[usenames]{color}
#+LATEX_HEADER: \usepackage{pstricks}
#+LATEX_HEADER: \usepackage{pgfplots}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage[europeanresistors,americaninductors]{circuitikz}
#+LATEX_HEADER: \usepackage{colortbl}
#+LATEX_HEADER: \usepackage{yfonts}
#+LATEX_HEADER: \usetikzlibrary{shapes,arrows}
#+LATEX_HEADER: \usetikzlibrary{positioning}
#+LATEX_HEADER: \usetikzlibrary{arrows,shapes}
#+LATEX_HEADER: \usetikzlibrary{intersections}
#+LATEX_HEADER: \usetikzlibrary{calc,patterns,decorations.pathmorphing,decorations.markings}
#+LATEX_HEADER: \usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
#+LATEX_HEADER: \setCJKmainfont[BoldFont=Evermore Hei]{Evermore Kai}
#+LATEX_HEADER: \setCJKmonofont{Evermore Kai}
 # +LATEX_HEADER: \xeCJKsetup{CJKglue=\hspace{0pt plus .08 \baselineskip }}
#+LATEX_HEADER: \usepackage{pst-node}
#+LATEX_HEADER: \usepackage{pst-plot}
#+LATEX_HEADER: \psset{unit=5mm}

#+startup: beamer
#+LaTeX_CLASS: beamer
# +LaTeX_CLASS_OPTIONS: [bigger]
#+latex_header: \usepackage{beamerarticle}
# +latex_header: \mode<beamer>{\usetheme{JuanLesPins}}
#+latex_header: \mode<beamer>{\usetheme{Frankfurt}}
#+latex_header: \mode<beamer>{\usecolortheme{dove}}
#+latex_header: \mode<article>{\hypersetup{colorlinks=true,pdfborder={0 0 0}}}

#+TITLE:  决策树学习
#+AUTHOR:    
#+EMAIL:
#+DATE:
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}

#+latex_header:\setbeamercovered{transparent}
#+BEAMER_FRAME_LEVEL: 3
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)







* 决策树表示法

- 决策树通过把实例从根结点排列（sort）到某个叶子结点来分类实例，叶子结点即为实例所属的分类。
- 树上的每一个结点指定了对实例的某个属性（attribute）的测试，并且该结点的每一个后继分支对应于该属性的一个可能值。
- 分类实例的方法是从这棵树的根结点开始，测试这个结点指定的属性，然后按照给定实例的该属性值对应的树枝向下移动。
- 这个过程再在以新结点为根的子树上重复。

* 决策树学习的适用问题

- 实例是由“属性-值”对（pair）表示的。实例是用一系列固定的属性（例如，Temperature）和它们的值（例如，Hot）来描述的。
   最简单的决策树学习中，每一个属性取少数的分离的值（例如，Hot、Mild、Cold）。
- 目标函数具有离散的输出值。决策树可给每个实例赋予一个布尔型的分类（例如，yes或no）。决策树方法很容易扩展到学习有两个以上输出值的函数。
   一种更强有力的扩展算法允许学习具有实数值输出的函数，尽管决策树在这种情况下的应用不太常见。
- 可能需要析取的描述（disjunctive description）。
- 训练数据可以包含错误。决策树学习对错误有很好的鲁棒性，无论是训练样例所属的分类错误还是描述这些样例的属性值错误。
- 训练数据可以包含缺少属性值的实例。

** A Tree to Predict C-Section Risk 

Learned from medical records of 1000 women

Negative examples are C-sections

#+BEGIN_EXAMPLE
  [833+,167-] .83+ .17-
  Fetal_Presentation = 1: [822+,116-] .88+ .12-
  | Previous_Csection = 0: [767+,81-] .90+ .10-
  | | Primiparous = 0: [399+,13-] .97+ .03-
  | | Primiparous = 1: [368+,68-] .84+ .16-
  | | | Fetal_Distress = 0: [334+,47-] .88+ .12-
  | | | | Birth_Weight < 3349: [201+,10.6-] .95+ .05-
  | | | | Birth_Weight >= 3349: [133+,36.4-] .78+ .22-
  | | | Fetal_Distress = 1: [34+,21-] .62+ .38-
  | Previous_Csection = 1: [55+,35-] .61+ .39-
  Fetal_Presentation = 2: [3+,29-] .11+ .89-
  Fetal_Presentation = 3: [8+,22-] .27+ .73-
#+END_EXAMPLE


* 基本的决策树学习算法
** ID3(Examples，Target_attribute，Attributes)
    Examples即训练样例集。
    Target_attribute是这棵树要预测的目标属性。
    Attributes是除目标属性外供学习到的决策树测试的属性列表。
    返回能正确分类给定Examples的决策树。
- 创建树的Root结点
- 如果Examples都为正，那么返回label =+ 的单结点树Root
- 如果Examples都为反，那么返回label =- 的单结点树Root
- 如果Attributes为空，那么返回单结点树Root，label=Examples中最普遍的Target_attribute值
- 否则
   - A←Attributes中分类Examples能力最好的属性
   - Root的决策属性←A
   - 对于A的每个可能值 $v_i$
          - 在Root下加一个新的分支对应测试 $A=v_i$
          - 令Examples_{v_i}为Examples中满足A属性值为 $v_i$ 的子集
             -  Examples_{v_i}是否为空？
                -  是则在这个新分支下加一个叶子结点，结点的label=Examples中最普遍的Target_attribute值
                -  否则在这个新分支下加一个子树ID3（, Target_attribute, Attributes-{A}）

** 最佳分类属性

** 熵（entropy）
给定包含关于某个目标概念的正反样例的样例集S，那么S相对这个布尔型分类的熵为：

\[ Entropy(S) \equiv  - p_{\oplus} \log_{2} p_{\oplus} -  p_{\ominus} \log_{2}p_{\ominus} \]

- $p_{\oplus}$ 是 $S$ 中正例的比例
- $p_{\ominus}$ 是 $S$ 中反例的比例 



熵确定了要编码集合 S 中任意成员（即以均匀的概率随机抽出的一个成员）的分类所需要的最少二进制位数。



** 信息增益度量

一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低。
一个属性A相对样例集合S的信息增益Gain(S,A)被定义为

\[ Gain(S,A) \equiv Entropy(S)\ - \sum_{v \in Values(A)} \frac{|S_{v}|}{|S|}Entropy(S_{v}) \]

其中 Values(A)是属性A所有可能值的集合，是S中属性A的值为v的子集（也就是， $S_v=\{s\in S|A(s)=v\}$）


** 例

| Day | Outlook  | Temperature | Humidity | Wind   | PlayTennis |
|-----+----------+-------------+----------+--------+------------|
| D1  | Sunny    | Hot         | High     | Weak   | No         |
| D2  | Sunny    | Hot         | High     | Strong | No         |
| D3  | Overcast | Hot         | High     | Weak   | Yes        |
| D4  | Rain     | Mild        | High     | Weak   | Yes        |
| D5  | Rain     | Cool        | Normal   | Weak   | Yes        |
| D6  | Rain     | Cool        | Normal   | Strong | No         |
| D7  | Overcast | Cool        | Normal   | Strong | Yes        |
| D8  | Sunny    | Mild        | High     | Weak   | No         |
| D9  | Sunny    | Cool        | Normal   | Weak   | Yes        |
| D10 | Rain     | Mild        | Normal   | Weak   | Yes        |
| D11 | Sunny    | Mild        | Normal   | Strong | Yes        |
| D12 | Overcast | Mild        | High     | Strong | Yes        |
| D13 | Overcast | Hot         | Normal   | Weak   | Yes        |
| D14 | Rain     | Mild        | High     | Strong | No         |


entropy(x)=-x*log2(x)-(1-x)*log2(1-x)
Gain(S,Outlook)=entropy(5/14)-5/14*entropy(2/5)-5/14*entropy(3/5)

** 属性选择的其它度量标准

增益比率 ( $GainRatio$ )

\[GainRatio(S,A) \equiv \frac{Gain(S,A)}{SplitInformation(S,A)} \]

\[ SplitInformation(S,A) \equiv - \sum_{i=1}^{c} \frac{|S_{i}|}{|S|} \log_{2}\frac{|S_{i}|}{|S|} \]

其中 $S_{i}$  是 $S$ 中属性 $A$ 的值为 $v_{i}$ 的子集
