% Created 2016-10-11 Tue 16:45
\documentclass{beamer}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{etex}
\usepackage{amsmath}
\usepackage{pstricks}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage[europeanresistors,americaninductors]{circuitikz}
\usepackage{colortbl}
\usepackage{yfonts}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{intersections}
\usetikzlibrary{calc,patterns,decorations.pathmorphing,decorations.markings}
\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\setCJKmainfont[BoldFont=Evermore Hei]{Evermore Kai}
\setCJKmonofont{Evermore Kai}
\usepackage{pst-node}
\usepackage{pst-plot}
\psset{unit=5mm}
\mode<beamer>{\usetheme{Frankfurt}}
\mode<beamer>{\usecolortheme{dove}}
\mode<article>{\hypersetup{colorlinks=true,pdfborder={0 0 0}}}
\AtBeginSection[]{\begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}}
\setbeamercovered{transparent}
\providecommand{\alert}[1]{\textbf{#1}}

\title{基于实例的学习}
\author{}
\date{}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.9.3f}}

\begin{document}

\maketitle

\begin{frame}
\frametitle{Outline}
\setcounter{tocdepth}{3}
\tableofcontents
\end{frame}







\section{简介}
\label{sec-1}
\begin{frame}
\frametitle{基于实例的学习（Instance Based Learning ）}
\label{sec-1-1}

\begin{itemize}
\item $k$ -近邻 ( $k$ -Nearest Neighbor)
\item 局部加权回归 （Locally weighted regression）
\item 径向基函数（Radial basis functions）
\item Case-based reasoning
\item Lazy and eager learning
\end{itemize}
\end{frame}
\section{KNN}
\label{sec-2}
\begin{frame}
\frametitle{K最近邻算法}
\label{sec-2-1}


关键思想： 只保存所有训练样例 $\langle x_i, f(x_i) \rangle$

\begin{itemize}
\item 最近邻
\begin{itemize}
\item 给定查询实例 $x_q$, 首选确定最近的训练实例 $x_n$,
\item 然后估计 $\hat{f}(x_q) \leftarrow f(x_n)$
\end{itemize}
\item $k$ - 最近邻
\begin{itemize}
\item 给定查询实例 Given $x_q$ , 最近的  $k$  个训练实例投票 (目标函数为离散值)
\item 最近的 $k$ 个训练样例的  $f$ 值取平均(实值目标函数)
        $$\hat{f}(x_{q}) \leftarrow  \frac{\sum_{i=1}^{k}f(x_{i})}{k}$$
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{何时使用最近邻算法}
\label{sec-2-2}


\begin{itemize}
\item 实例映身到空间 $\Re^n$ 中的点
\item 每实例少于 20 个属性
\item 大量训练数据
\item 优点:
\begin{itemize}
\item 训练快
\item 可学习复杂函数
\item 不损失信息
\end{itemize}
\item 缺点:
\begin{itemize}
\item 查询慢
\item 易受不相关属性干扰
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Voronoi Diagram}
\label{sec-2-3}


\includegraphics[width=.9\linewidth]{./image/knn-f1.png}
\end{frame}
\begin{frame}
\frametitle{Behavior in the Limit}
\label{sec-2-4}


设 $p(x)$ 定义为实例 $x$ 应被标记为 1 (positive) 而不是  0 (negative) 的概率。

最近邻: 
\begin{itemize}
\item 当训练样例数量 $\rightarrow \infty$, 逼近 Gibbs 算法
\item Gibbs: 按概率 $p(x)$ 预测  1, 否则 0
\end{itemize}

$k$ -最近邻: 
\begin{itemize}
\item 当训练样例数量 $\rightarrow \infty$ 且 $k$ 值较大, 逼近贝叶斯最优分类器
\item Bayes optimal: 当 $p(x)>.5$ 预测 1, 否则 0
\end{itemize}


注意： 期望错误率 Gibbs 至多是 Bayes optimal 的两倍。
\end{frame}
\begin{frame}
\frametitle{距离加权KNN算法（Distance-Weighted $k$ NN）}
\label{sec-2-5}


\begin{itemize}
\item 也许想让距离近的权重大些
  $$\hat{f}(x_{q}) \leftarrow  \frac{\sum_{i=1}^{k} w_{i} f(x_{i})}{\sum_{i=1}^{k} w_{i}}$$
  其中
\begin{itemize}
\item $$w_{i} \equiv \frac{1}{d(x_{q}, x_{i})^{2}}$$
\item $d(x_{q}, x_{i})$ 是 $x_{q}$ 与 $x_{i}$ 之间的距离
\end{itemize}
\item 注意：现在它可以使用所有训练样例而不仅是 $k$ 个。(Shepard's method)
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Curse of Dimensionality}
\label{sec-2-6}

\begin{itemize}
\item 考虑把k-近邻算法应用到这样一个问题：
\begin{itemize}
\item 每个实例由20个属性描述，但在这些属性中仅有2个与它的分类是有关。
\item 在这种情况下，这两个相关属性的值一致的实例可能在这个20维的实例空间中相距很远。
\item 结果，依赖这20个属性的相似性度量会误导k-近邻算法的分类。
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Curse of Dimensionality}
\label{sec-2-7}

\begin{itemize}
\item \em{ Curse of dimensionality} : 高维空间 $X$ 中最近邻易被误导
\item 一种解决方法
\begin{itemize}
\item 按权重 $z_j$ 伸展第 $j$ 个坐标轴 ,  其中 $z_1, \ldots, z_n$ 按最小预测误差选取
\item 使用交叉验证（ cross-validation）自动选取权重 $z_1, \ldots, z_n$
\item 设置 $z_j$ 为0可消除第 $j$ 维影响
\end{itemize}
\end{itemize}
\end{frame}
\section{局部加权回归}
\label{sec-3}
\begin{frame}
\frametitle{局部加权回归（Locally Weighted Regression）}
\label{sec-3-1}

\begin{itemize}
\item kNN 为每个查询点 $x_q$ 构造了 $f$ 的局部逼近
\item 局部加权回归为包含 $x_q$ 的区域显示地构造逼近函数 $\hat{f}(x)$
\begin{itemize}
\item 对 $k$ 个近邻 拟合线性函数
\item 拟合二次函数
\item 分段逼近 $f$
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{误差准则:}
\label{sec-3-2}

\begin{itemize}
\item $k$ 近邻的误差平方和最小化
      $$E_{1}(x_q) \equiv \frac{1}{2} \sum_{x \in\ k\ nearest\ nbrs\ of\ x_q} (f(x)- \hat{f}(x))^2$$
\item 使整个训练样例集合D上的误差平方和最小化，但对每个训练样例加权，权值为关于相距xq距离的某个递减函数K：
      $$E_{2}(x_q) \equiv \frac{1}{2} \sum_{x \in D} (f(x) - \hat{f}(x))^2 K(d(x_{q}, x))$$
\end{itemize}
\end{frame}
\section{RBF Networks}
\label{sec-4}
\begin{frame}
\frametitle{Radial Basis Function Networks}
\label{sec-4-1}

\begin{itemize}
\item 全局逼近目标函数 , 是局部逼近的线性组合
\item 另一种神经网络
\item 与距离加权回归有密切联系，但属于积极（eager）方法，而不是消极（lazy）方法。
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Radial Basis Function Networks}
\label{sec-4-2}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{block}{网络}
\label{sec-4-2-1}

\includegraphics[width=0.9\textwidth]{./image/rbf2.png}
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}

\item 说明\\
\label{sec-4-2-2}%
其中 $a_i(x)$ 是描述实例 $x$ 的属性 , 且有
$$f(x) =  w_0 + \sum_{u=1}^{k} w_u K_u(d(x_u,x))$$

 $K_u(d(x_u,x))$ 通常可选为：
$$K_u(d(x_u,x)) = e^{- \frac{1}{2 \sigma_u^2}d^2(x_u,x)}$$

\end{itemize} % ends low level
\end{column}
\end{columns}
\end{frame}
\begin{frame}
\frametitle{Training Radial Basis Function Networks}
\label{sec-4-3}


\begin{itemize}
\item Q1: 核函数 $K_u(d(x_u,x))$ 的 $x_u$ 如何选取？
\begin{itemize}
\item 均匀分布在实例空间中
\item 或使用训练实例(反映了实例分布)
\end{itemize}
\item Q2: 如何训练权重 (假设是 Gaussian $K_u$)
\begin{itemize}
\item 首先为每个$K_u$ 选择方差 (与均值)
\begin{itemize}
\item 例如，使用 EM 算法
\end{itemize}
\item 然后固定  $K_u$ , 训练线性网络层
\begin{itemize}
\item 拟合线性函数
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}
\section{Case-Based Reasoning}
\label{sec-5}
\begin{frame}[fragile]
\frametitle{Case-Based Reasoning}
\label{sec-5-1}

 当 $X \neq \Re^n$ 时 应用基于实例的学习 （需要不同的“距离”度量）

基于案例的推理——基于实例的推理应用于符号逻辑描述

\begin{verbatim}
((user-complaint error53-on-shutdown)
 (cpu-model PowerPC)
 (operating-system Windows)
 (network-connection PCIA)
 (memory 48meg)
 (installed-applications Excel Netscape VirusScan)
 (disk 1gig)
 (likely-cause ???))
\end{verbatim}
\end{frame}
\begin{frame}
\frametitle{Case-Based Reasoning in CADET}
\label{sec-5-2}


\begin{itemize}
\item CADET: 存储了 75 个机械设置样例
\begin{itemize}
\item 每个训练样例: $\langle$ qualitative function, mechanical-structure $\rangle$
\item 新查询: desired function,
\item 目标: mechanical structure for this function
\end{itemize}
\item 距离度量: match qualitative function descriptions
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Case-Based Reasoning in CADET}
\label{sec-5-3}

\includegraphics[width=0.8\textwidth]{./image/cbr.png}
\end{frame}
\begin{frame}
\frametitle{与KNN区别}
\label{sec-5-4}

基于案例的推理系统区别于k-近邻这样的方法的若干一般特征：
\begin{itemize}
\item 实例或案例可以用丰富的符号描述表示，就像CADET中使用的功能图。
   这可能需要不同于欧氏距离的相似性度量，比如两个功能图的最大可共享子图的大小。
\item 检索到的多个案例可以合并形成新问题的解决方案。
   这与k-近邻方法相似——多个相似的案例用来构成对新查询的回答。
   然而，合并多个检索到的案例的过程与k-近邻有很大不同，它依赖于知识推理而不是统计方法。
\item 案例检索、基于知识的推理和问题求解间是紧密耦合在一起的。
   例如CADET系统在尝试找到匹配的案例过程中，它使用有关物理感应的一般知识重写了功能图。
\end{itemize}
\end{frame}
\section{Lazy and Eager Learning}
\label{sec-6}
\begin{frame}
\frametitle{Lazy and Eager Learning}
\label{sec-6-1}

\begin{itemize}
\item Lazy: 消极方法等到查询实例 $x_q$ 时从训练数据 $D$ 中泛化
\begin{itemize}
\item $k$ -Nearest Neighbor, Case based reasoning
\end{itemize}
\item Eager: 积极方法在见到查询实例 $x_q$ 前，已经选进行了泛化（选取了对目标函数的（全局）逼近）。
\begin{itemize}
\item Radial basis function networks,  ID3, Backpropagation, NaiveBayes, $\ldots$
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{积极学习的和消极学习之间的差异}
\label{sec-6-2}

\begin{itemize}
\item 积极学习的和消极学习之间的差异意味着对目标函数的全局逼近和局部逼近的差异。
\begin{itemize}
\item 消极的学习器可以通过很多局部逼近的组合（隐含地）表示目标函数，
\item 积极的学习器必须在训练时提交单个的全局逼近。
\item 对于同样的 $H$, 消极的学习器可表达更复杂的函数 (如 $H$ = 线性函数)
\end{itemize}
\end{itemize}
\end{frame}

\end{document}
