\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
%\begin{CJK*}{GBK}{song}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\def\lecturename{ System Identification }

\title{\insertlecture}

\author{Xing Chao}

\institute
{
   Institute of Astronautics, Northwestern Polytechnical University 
}

%\mode<presentation>{\subject{嵌入式系统}}

%  start a lecture  --------------------------
\lecture[LS]{ Least squares identification }{}
\subtitle{ Improved algorithm }
\date{2017}


%\setbeamertemplate{background}{\pgfimage[width=\paperwidth,height=\paperheight]{image/flower}}
%\setbeamercovered{transparent}
%\mode<presentation>{\beamerdefaultoverlayspecification{<+->}}

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}{Contents}
\tableofcontents
\end{frame}

\section{auxiliary variable method}
\begin{frame}{auxiliary variable method}
\begin{itemize}
\item The identification accuracy is higher than the basic least squares estimation method；
\item computate easily；
\item  Asymptotically unbiased estimate ；
\item Construction of auxiliary variable matrix。
\end{itemize}
\end{frame}

\begin{frame}{Principle of auxiliary variable method}
\begin{eqnarray*}
Y &=& \Phi \theta+\xi  \\
\only<1>{
\Phi^T Y &=& \Phi^T \Phi \theta+\Phi^T\xi  \\
(\Phi^T \Phi)^{-1} \Phi^T Y &=& (\Phi^T \Phi)^{-1}\Phi^T \Phi \theta+(\Phi^T \Phi)^{-1}\Phi^T\xi \\
(\Phi^T \Phi)^{-1} \Phi^T Y &=& \theta+(\Phi^T \Phi)^{-1}\Phi^T\xi \\
}
\only<2>{
Z^T Y &=& Z^T \Phi \theta+Z^T\xi  \\
(Z^T \Phi)^{-1} \Phi^T Y &=& (Z^T \Phi)^{-1}\Phi^T \Phi \theta+(Z^T \Phi)^{-1}Z^T\xi \\
(Z^T \Phi)^{-1} \Phi^T Y &=& \theta+(Z^T \Phi)^{-1}Z^T\xi 
}
\end{eqnarray*}
\only<2>{
where：
\begin{eqnarray*}
E(Z^T\xi) &=& 0 \\
E(Z^T\Phi) &=& Q 
\end{eqnarray*}
where $Q$ Nonsingular 。
}
\end{frame}

\begin{frame}{ Asymptotically unbiased }
\begin{eqnarray*}
E[\hat\theta_{IV}] &=& E[(Z^T\Phi)^{-1}Z^T Y] \\
&=& E[(Z^T\Phi)^{-1}Z^T (\Phi\theta +\xi)] \\
&=& \theta+E[(Z^T\Phi)^{-1}Z^T \xi] \\
\lim_{N\to\infty}E[\hat\theta_{IV}] &=& \theta+ E[(Z^T\Phi)^{-1}]E[Z^T \xi] \\
&=& \theta 
\end{eqnarray*}
\end{frame}

\begin{frame}{The construction method of auxiliary variable method}
\begin{itemize}
\item Recursive auxiliary variable parameter estimation method 
\item Adaptive filtering method
\item Pure lag method 
\item Taly principle method
\end{itemize}
\end{frame}

\begin{frame}{Recursive auxiliary variable parameter estimation method:$Z$}
\begin{eqnarray*}
\hat Y &=& Z \hat\theta  \\
Z &=& \begin{bmatrix}
-\hat y_n   & \cdots &  -\hat y_1  & u_{n+1} & \cdots & u_1 \\
\vdots      &        & \vdots      & \vdots  &        & \vdots \\
-\hat y_{n+N-1}   & \cdots &  -\hat y_N  & u_{n+N} & \cdots & u_N 
\end{bmatrix}
\end{eqnarray*}
\end{frame}

\begin{frame}{Recursive auxiliary variable parameter estimation method:process}
\begin{itemize}
\item initialize： use basic least squares method to estimate $\hat\theta$, let $Z=\Phi$，
\item  Recurse：
\begin{itemize}
\item update $Z$
$$\hat Y=Z\hat\theta$$
\item compute $\hat\theta$
$$\hat\theta = (Z^T\Phi)^{-1}Z^T Y$$
\item iterate until $\hat\theta$ converges。
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{ Adaptive filtering method }
 On the basis of recursive auxiliary variable parameter estimation method ,let:
$$\hat\theta_k=(1-\alpha)\hat\theta_{k-1}+\alpha\hat\theta_{k-d}$$
\begin{description}
\item[$\alpha$:] $\in [0.01,0.1]$
\item[$d$:] $\in [0,10],$
\end{description}
\end{frame}

\begin{frame}{Pure lag method}
\begin{eqnarray*}
\hat y_k &=& u_{k-m}
\end{eqnarray*}
When $d=n$，there is：

\begin{eqnarray*}
Z&=&\begin{bmatrix}
-u_0 & \cdots & -u_{1-n} & u_{n+1} & \cdots & u_1 \\
-u_1 & \cdots & -u_{2-n} & u_{n+2} & \cdots & u_2 \\
\vdots &      & \vdots   &  \vdots &        \vdots \\
-u_{N-1} & \cdots & -u_{N-n} & u_{n+N} & \cdots & u_2 
\end{bmatrix}
\end{eqnarray*}
\end{frame}

\begin{frame}{Tally principle}
If noise $\xi_k$  is refered as output of this model :
$$\xi_k = c(z^{-1})n_k$$
where $n_k$ is uncorrelated ramdom noise with zero mean。and：
$$c(z^{-1}) = 1+c_1 z^{-1}+\cdots +c_m z^{-m}$$
then, let：
\begin{eqnarray*}
\hat y_k &=& y_{k-m}\\
Z&=&\begin{bmatrix}
-y_{n-m} & \cdots & -y_{1-m} & u_{n+1} & \cdots & u_1 \\
-y_{n+1-m} & \cdots & -y_{2-m} & u_{n+2} & \cdots & u_2 \\
\vdots &      & \vdots   &  \vdots &        \vdots \\
-y_{n+N-1-m} & \cdots & -y_{N-m} & u_{n+N} & \cdots & u_2 
\end{bmatrix}
\end{eqnarray*}
\end{frame}

\begin{frame}{Recursive auxiliary variable method }
\begin{eqnarray*}
\onslide<1->{
\hat\theta_N &=& P_N Z_N^T Y_N \\
P_N &=& (Z_N^T\Phi_N)^{-1} \\
\hat\theta_{N+1} &=& P_{N+1} Z_{N+1}^T Y_{N+1} \\
}
\onslide<2>{
P_{N+1} &=& \left(\begin{bmatrix}Z_N^T  & Z_{N+1}\end{bmatrix}\begin{bmatrix}\Phi_N \\ \Psi_{N+1}^T \end{bmatrix}\right)^{-1} \\
&=& (P_N^{-1}+Z_{N+1}\Psi_{N+1}^T)^{-1} \\
\Psi_{N+1} &=& \begin{bmatrix}-y_{n+N} & \cdots & -y_{N+1} & u_{n+N+1} & \cdots & u_{N+1}\end{bmatrix}^T \\
z_{N+1} &=& \begin{bmatrix}-\hat y_{n+N} & \cdots & -\hat y_{N+1} & u_{n+N+1} & \cdots & u_{N+1}\end{bmatrix}^T 
}
\end{eqnarray*}
\end{frame}

\begin{frame}{ Recursive auxiliary variable method }
 By using the inverse lemma of matrix, the recursive formula can be deduced ：
\begin{eqnarray*}
\hat\theta_{N+1} &=& \hat\theta_{N}+K_{N+1}(y_{N+1}-\psi_{N+1}^T\hat\theta_N) \\
P_{N+1} &=& P_N - K_{N+1}\Psi_{N+1}^T P_N \\
K_{N+1} &=& P_N z_{N+1}(1+\Psi_{N+1}^T P_N z_{N+1})^{-1} 
\end{eqnarray*}
\begin{itemize}
\item  Select initial parameters by reference to recursive least square method 
\item is sensitive to initial value $P_0$，it is better to use recursive least squares methods with first 50\textasciitilde 100  point，then use auxiliary variable method.
\end{itemize}
\end{frame}

\section{ Generalized least square method }
\begin{frame}{ Generalized least square method }
\begin{itemize}
\item  The filtering model is established to whiten the data 
\item  The method is complex and with heavy computation
%\item 渐近无偏估计，估计效果较好
\item  The convergence of the iterative algorithm is not proved 
\end{itemize}
\end{frame}

\begin{frame}{ Generalized least squares: system model }
\begin{eqnarray*}
\only<1>{
a(z^{-1}) y_k &=& b(z^{-1}) u_k + \xi_k \\
f(z^{-1}) &=& 1+f_1 z^{-1}+\cdots+f_m z^{-m}  \\
\xi_k &=& \frac{1}{f(z^{-1})}\varepsilon_k \\
f(z^{-1})\xi_k &=& \varepsilon_k \\
\xi_k &=& -f_1\xi_{k-1} -\cdots -f_m\xi_{k-m}+\varepsilon_k \\
}
\only<2>{
a(z^{-1}) f(z^{-1})y_k &=& b(z^{-1}) f(z^{-1}) u_k + \varepsilon_k \\
a(z^{-1}) \bar y_k &=& b(z^{-1}) \bar u_k  + \varepsilon_k \\
\bar y_k &=& f(z^{-1})y_k \\
&=& y_k+f_1 y_{k-1}+\cdots + f_m y_{k-m}\\
\bar u_k &=& f(z^{-1}) u_k \\
&=& u_k + f_1 u_{k-1} + \cdots + f_m u_{k-m}
}
\end{eqnarray*}
\end{frame}

\begin{frame}{ Generalized least squares method: noise model parameter estimation }
\begin{eqnarray*}
\xi &=&\Omega f + \varepsilon \\
\xi &=& \begin{bmatrix}\xi_{n+1} & \xi_{n+2} & \cdots & \xi_{n+N}\end{bmatrix}^T \\
f &=& \begin{bmatrix}f_1 & f_2 & \cdots & f_m\end{bmatrix}^T \\
\varepsilon &=& \begin{bmatrix}\varepsilon_{n+1} & \varepsilon_{n+2} &\cdots& \varepsilon_{n+N}\end{bmatrix}^T \\
\Omega &=& \begin{bmatrix}
-\xi_n & \cdots & -\xi_{n+1-m} \\
-\xi_{n+1} & \cdots & -\xi_{n+2-m} \\
\vdots &        &\vdots \\
-\xi_{n+N-1} & \cdots & -\xi_{n+N-m} 
\end{bmatrix} \\
\hat f &=& (\Omega^T\Omega)^{-1}\Omega^T\xi
\end{eqnarray*}
\end{frame}

\begin{frame}{ Generalized least squares: process }
\begin{itemize}
\item initialize，let $$\hat f(z^{-1}) =1$$
\item iterate
\begin{itemize}
\item filtering：
\begin{eqnarray*}
\bar y_k &=& \hat f(z^{-1}) y_k \\
\bar u_k &=& \hat f(z^{-1}) u_k
\end{eqnarray*}
\item  Least square estimation ：$$\hat\theta = (\bar\Phi^T\bar\Phi)^{-1}\bar\Phi^T \bar Y  $$
\item residue：$$\hat\xi = Y-\Phi\hat\theta  $$
\item use residue $\hat\xi$ instead of  $\xi$ to compute $\hat f$：$$\hat f = (\hat\Omega^T\hat\Omega)^{-1}\hat\Omega^T e $$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{ Recursive generalized least squares method }
\begin{itemize}
\item include resursive estimate of parameter $\hat\theta$ and noise model parameter $\hat f$
\item  The results of offline and recursive calculation are not exactly the same 
\item process：
\begin{itemize}
\item  Initialization, and the initial value is selected by referring to recursive least square 
\item filtering, compute new value of $\bar y_k,\bar u_k$
\item  compute $\hat\theta$ and $\hat f$ by using recursive least square algorithm
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{ Recursive generalized least squares method }
\begin{itemize}
\item initialize：\
\begin{eqnarray*}
\hat\theta_0 &=& 0 \\
P_0^{(\theta)} &=& c_1^2 I \\
\hat f_{(0)} &=& 0 \\
P_0^{(f)} &=& c_2^2 I \\
\end{eqnarray*}
\item filtering
\begin{eqnarray*}
\bar y_{N+1} &=& \hat f_{(N)}(z^{-1}) y_{N+1} \\
&=& \hat f_{(N)}(z^{-1}) y_{(n+N+1)} \\
\bar u_{N+1} &=& \hat f_{(N)}(z^{-1}) u_{N+1} \\
&=& \hat f_{(N)}(z^{-1}) u_{(n+N+1)} 
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}{ Recursive generalized least squares method }
\begin{itemize}
\item compute $\hat\theta$
\begin{eqnarray*}
\hat\theta_{N+1} &=& \hat\theta_N+K_{N+1}^{(\theta)}(\bar y_{N+1}-\bar\Psi_{N+1}^T\hat\theta_N) \\
K_{N+1}^{(\theta)}&=& P_N^{(\theta)}\bar\Psi_{N+1}(1+\bar\Psi_{N+1}^T P_N^{(\theta)}\bar\Psi_{N+1})^{-1} \\
P_{N+1}^{(\theta)} &=& P_N^{(\theta)}-K_{N+1}^{(\theta)}\bar\Psi_{N+1}^T P_N^{(\theta)} \\
\bar\Psi_{N+1} &=& \begin{bmatrix}-\bar y_{n+N} &\cdots & -\bar y_{N+1} & \bar u_{n+N+1} &\cdots & \bar u_{N+1} \end{bmatrix}^T 
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}{ Recursive generalized least squares method }
\begin{itemize}
\item compute residue $\hat\xi_{N+1}$
\begin{eqnarray*}
\hat\xi_{N+1}&=& y_{N+1}-\Psi_{N+1}\hat\theta_{N+1}
\end{eqnarray*}
\item compute $\hat f$
\begin{eqnarray*}
\hat f_{N+1} &=& \hat f_N+K_{N+1}^{(f)}(\hat\xi_{N+1}-\hat\omega_{N+1}^T\hat f_N) \\
K_{N+1}^{(f)}&=& P_N^{(f)}\hat\omega_{N+1}(1+\hat\omega_{N+1}^T P_N^{(f)}\hat\omega_{N+1})^{-1} \\
P_{N+1}^{(f)} &=& P_N^{(f)}-K_{N+1}^{(f)}\hat\omega_{N+1}^T P_N^{(f)}  \\
\hat\omega_{N+1} &=& \begin{bmatrix}-\hat\xi_{n+N} &\cdots & -\hat\xi_{n+N+1-m}\end{bmatrix}
\end{eqnarray*}
\end{itemize}
\end{frame}

\section{Hsia method}

\begin{frame}{Hsia method}
\begin{itemize}
\item   Alternately solving system model and noise model parameters 
\item   It can be divided into two types: Hsia correction method and Hsia improvement method 
\item   Recursive algorithm can be extended to MIMO system 
\item   There is no need to filter the data repeatedly, so the calculation efficiency is relatively high
\item   The estimation result is relatively good
\end{itemize}
\end{frame}


\begin{frame}{ Method: record system model }
\begin{eqnarray*}
\onslide<1->{
a(z^{-1})y(k) &=& b(z^{-1})u(k)+\xi_{k}\\
\xi_{k} &=& \frac{\varepsilon(k)}{f(z^{-1})} \\
a(z^{-1}) &=& 1 + a_1 z^{-1} + \cdots + a_n z^{-n} \\
b(z^{-1}) &=& b_0 + b_1 z^{-1} + \cdots + b_n z^{-n} \\
f(z^{-1}) &=& 1 + f_1 z^{-1} + \cdots + f_m z^{-m}  \\
}
\onslide<2>{
\xi_{k} &=& (1-f( z^{-1})\xi_k+\varepsilon_k \\
a(z^{-1})y(k) &=& b(z^{-1})u(k)+(1-f(z^{-1}))\xi_{k}+\varepsilon_k
}
\end{eqnarray*}
\end{frame}

\begin{frame}{ Method: the system model of vector xias }
\begin{eqnarray*}
y_{N} &=& y_{(n+N)} \\
&=& \Psi_N^T\theta + \omega_N^T f+\varepsilon_{N} \\
f &=&\begin{bmatrix} f_1 &\cdots & f_m\end{bmatrix}^T \\
\Psi_N &=&\begin{bmatrix}-y_{(n+N-1)} & \cdots & -y_{(N)} & u_{(n+N)} &\cdots & u_{(N)} \end{bmatrix}^T \\
\omega_N &=&\begin{bmatrix} -\xi_{(n+N-1)} & \cdots & -\xi_{(n+N-m)} \end{bmatrix}^T  
\end{eqnarray*}
\end{frame}

\begin{frame}{ Method:  parameters }
\begin{eqnarray*}
\begin{bmatrix} y_1 \\ \vdots \\ y_{N} \end{bmatrix}&=& 
\begin{bmatrix} \Psi_1^T & \omega_1^T  \\
\vdots & \vdots \\
\Psi_N^T & \omega_N^T
\end{bmatrix} \begin{bmatrix}\theta \\ f \end{bmatrix} + 
\begin{bmatrix} \varepsilon_1 \\ \vdots \\ \varepsilon_N \end{bmatrix}\\
Y &=& \begin{bmatrix} \Phi & \Omega \end{bmatrix} \begin{bmatrix}\theta \\ f \end{bmatrix} + \varepsilon \\
\begin{bmatrix}\hat\theta \\ \hat f \end{bmatrix} &=& \begin{bmatrix}\Phi^T\Phi & \Phi^T\Omega \\ \Omega^T\Phi & \Omega^T\Omega \end{bmatrix}^{-1}\begin{bmatrix}\Phi^T Y \\ \Omega^T Y\end{bmatrix}
\end{eqnarray*}
\end{frame}


\begin{frame}{ Method: deviation correction method }
inverse by ulsing block matrix ：
\begin{eqnarray*}
\begin{bmatrix}\hat\theta \\ \hat f \end{bmatrix} &=& \begin{bmatrix}\Phi^T\Phi & \Phi^T\Omega \\ \Omega^T\Phi & \Omega^T\Omega \end{bmatrix}^{-1}\begin{bmatrix}\Phi^T Y \\ \Omega^T Y\end{bmatrix} \\
&=& \begin{bmatrix}P_N \Phi^T Y - P_N\Phi^T \Omega D^{-1} \Omega^T M Y \\ D^{-1}\Omega^T M Y\end{bmatrix} \\
&=& \begin{bmatrix}\hat\theta_{LS} - P_N\Phi^T \Omega \hat f \\ D^{-1}\Omega^T M Y\end{bmatrix} \\
P_N &=& (\Phi^T\Phi)^{-1} \\
M &=& I-\Phi(\Phi^T\Phi)^{-1}\Phi^T \\
D &=& \Omega^T M \Omega
\end{eqnarray*}
\end{frame}


\begin{frame}{ Method:  deviation correction method  iteration steps }
\begin{itemize}
\item  Initialization: computing basic least squares estimation 
\begin{eqnarray*}
\hat\theta &=& (\Phi^T\Phi)^{-1} \Phi^T Y
\end{eqnarray*}
\item iterate
\begin{itemize}
\item compute residual $\hat\xi$ to construct $\hat\Omega$
\begin{eqnarray*}
\hat\xi &=& Y- \Phi\hat\theta
\end{eqnarray*}
\item compute $\hat f$ to correct $\hat\theta$
\begin{eqnarray*}
\hat f &=& D^{-1}\hat\Omega^T M Y \\
\hat\theta &=& \hat\theta-(\Phi^T\Phi)^{-1} \Phi^T \hat\Omega \hat f
\end{eqnarray*}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{ Method: the modified method }
use $\hat\theta$ instead of $\theta$：
\begin{eqnarray*}
Y &=& \begin{bmatrix} \Phi & \Omega \end{bmatrix} \begin{bmatrix}\hat\theta \\ f \end{bmatrix} + \varepsilon \\
&=& \Phi \hat\theta+ \Omega  f  + \varepsilon \\
Y- \Phi \hat\theta &=& \Omega f  + \varepsilon 
\end{eqnarray*}
obtained least squares estimate of $f $：
\begin{eqnarray*}
\hat f &=& (\hat\Omega^T \hat\Omega)^{-1}\hat\Omega^T(Y- \Phi \hat\theta) \\
\hat\theta &=& \hat\theta-(\Phi^T\Phi)^{-1} \Phi^T \Omega \hat f
\end{eqnarray*}
\end{frame}

\begin{frame}{ Method: the modified method  iteration steps }
\begin{itemize}
\item initialize： Computational basic least squares estimation 
\begin{eqnarray*}
\hat\theta &=& (\Phi^T\Phi)^{-1} \Phi^T Y
\end{eqnarray*}
\item iterate
\begin{itemize}
\item compute residual $\hat\xi$ to construct $\hat\Omega$
\begin{eqnarray*}
\hat\xi &=& Y- \Phi\hat\theta
\end{eqnarray*}
\item compute $\hat f$ to correct $\hat\theta$
\begin{eqnarray*}
\hat f &=& (\hat\Omega^T \hat\Omega)^{-1}\hat\Omega^T(Y- \Phi \hat\theta) \\
\hat\theta &=& \hat\theta-(\Phi^T\Phi)^{-1} \Phi^T \hat\Omega \hat f
\end{eqnarray*}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{ Method: the recursive method }
\begin{eqnarray*}
\tilde\Phi &=&\begin{bmatrix}\Phi & \hat\Omega\end{bmatrix} \\
\tilde\theta &=&\begin{bmatrix}\hat\theta \\ f \end{bmatrix} \\
\tilde\theta_{N+1}^T &=& \tilde\theta_N + K_{N+1} (y_{N+1}-\tilde\Psi_{N+1}^T\tilde\theta_N) \\
P_{N+1} &=& P_N -K_{N+1}\tilde\Psi_{N+1}^T P_N \\
K_{N+1} &=& P_N \tilde\Psi_{N+1}^T (1+\tilde\Psi_{N+1}^T P_N \tilde\Psi_{N+1})^{-1}
\end{eqnarray*}
其中：
\begin{eqnarray*}
y_{N} &=& \tilde \Psi_N^T\tilde\theta + \hat\varepsilon_{(n+N)} \\
\tilde\Psi_N &=&\begin{bmatrix}\Psi_N^T & \hat\omega_N^T \end{bmatrix}^T \\
\Psi_N &=& \begin{bmatrix} -y_{(n+N-1)} & \cdots & -y_{(N)} & u_{(n+N)} &\cdots & u_{(N)} \end{bmatrix}^T \\
\hat\omega_N &=&\begin{bmatrix} \hat\xi_{(n+N-1)} & \cdots & \hat\xi_{(n+N-m)} \end{bmatrix}^T \\
\hat\xi_k &=& y_k-\Psi_k \hat\theta
\end{eqnarray*}
\end{frame}

\section{ Augmented matrix method }
\begin{frame}{ Augmented matrix method }
\begin{itemize}
\item   The noise model parameters are extended to the identified parameter vectors 
\item   Simultaneous identification of system parameters and noise parameters 
%\item  无偏估计方法
\item   It is widely used and has a good convergence 
\item   Recursive methods are often used in practical algorithms 
\end{itemize}
\end{frame}

\begin{frame}{ Augmented matrix method: system model }
\begin{eqnarray*}
a(z^{-1})y(k) &=& b(z^{-1})u(k)+c(z^{-1})\varepsilon(k) \\
a(z^{-1}) &=& 1 + a_1 z^{-1} + \cdots + a_n z^{-n} \\
b(z^{-1}) &=& b_0 + b_1 z^{-1} + \cdots + b_n z^{-n} \\
c(z^{-1}) &=& 1 + c_1 z^{-1} + \cdots + c_n z^{-n} 
\end{eqnarray*}
\end{frame}

\begin{frame}{ Augmented matrix method: system model vector representation }
\begin{eqnarray*}
y_{N} &=& y_{(n+N)} \\
&=& \Psi_N^T\theta +\varepsilon_{(n+N)} \\
&=& \begin{bmatrix} \Psi_{N,(y,u)}^T & \Psi_{N,\xi}^T\end{bmatrix}\begin{bmatrix}\theta_{(y,u)} \\ \theta_\xi \end{bmatrix} + \varepsilon_N \\
\theta &=&\begin{bmatrix}\theta_{(y,u)} & \theta_\xi\end{bmatrix}^T \\
\theta_{(y,u)} &=&\begin{bmatrix}a_1 & \cdots & a_n & & b_0 & \cdots & b_n \end{bmatrix}^T \\
\theta_\xi &=&\begin{bmatrix} c_1 &\cdots & c_n\end{bmatrix}^T \\
\Psi_N &=&\begin{bmatrix} \Psi_{N,(y,u)} &\Psi_{N,\xi} \end{bmatrix}^T \\
\Psi_{N,(y,u)} &=&\begin{bmatrix}-y_{(n+N-1)} & \cdots & -y_{(N)} & u_{(n+N)} &\cdots & u_{(N)} \end{bmatrix}^T \\
\Psi_{N,\xi} &=&\begin{bmatrix} \varepsilon_{(n+N-1)} & \cdots & \varepsilon_{(N)} \end{bmatrix}^T  
\end{eqnarray*}
\end{frame}

\begin{frame}{ Augmented matrix method: Parameter Solving }
\begin{eqnarray*}
\begin{bmatrix} y_1 \\ \vdots \\ y_{N} \end{bmatrix}&=& 
\begin{bmatrix} \Psi_{1,(y,u)}^T & \Psi_{1,\xi}^T  \\
\vdots & \vdots \\
\Psi_{N,(y,u)}^T & \Psi_{N,\xi}^T
\end{bmatrix} \begin{bmatrix}\theta_{(y,u)} \\ \theta_\xi \end{bmatrix} + 
\begin{bmatrix} \varepsilon_1 \\ \vdots \\ \varepsilon_N \end{bmatrix}\\
Y &=& \begin{bmatrix} \Phi_{(y,u)} & \Phi_\xi \end{bmatrix} \begin{bmatrix}\theta_{(y,u)} \\ \theta_\xi \end{bmatrix} + \varepsilon \\
\begin{bmatrix}\hat\theta_{(y,u)} \\ \hat\theta_\xi \end{bmatrix} &=& \begin{bmatrix}\Phi_{(y,u)}^T\Phi_{(y,u)} & \Phi_{(y,u)}^T\Phi_\xi \\ \Phi_\xi^T\Phi_{(y,u)} & \Phi_\xi^T\Phi_\xi \end{bmatrix}^{-1}\begin{bmatrix}\Phi_{(y,u)}^T Y \\ \Phi_\xi^T Y\end{bmatrix}
\end{eqnarray*}
\end{frame}

\begin{frame}{ Augmented matrix method: recursive equations }
use $\hat\varepsilon$ instead of $\varepsilon$：
\begin{eqnarray*}
y_{N} &=& \hat \Psi_N^T\hat\theta +\hat\varepsilon_{(n+N)} \\
\hat\Psi_N &=&\begin{bmatrix}-y_{(n+N-1)} & \cdots & -y_{(N)} & u_{(n+N)} &\cdots & u_{(N)} & \hat\epsilon_N^T \end{bmatrix}^T \\
\hat\epsilon_N &=&\begin{bmatrix} \hat\varepsilon_{(n+N-1)} & \cdots & \hat\varepsilon_{(N)} \end{bmatrix}^T \\
\end{eqnarray*}
 Available recurrence formula ：
\begin{eqnarray*}
\hat\theta_{N+1}^T &=& \hat\theta_N + K_{N+1} (y_{N+1}-\hat\Psi_{N+1}^T\hat\theta_N) \\
P_{N+1} &=& P_N -K_{N+1}\hat\Psi_{N+1}^T P_N \\
K_{N+1} &=& P_N \hat\Psi_{N+1}^T (1+\hat\Psi_{N+1}^T P_N \hat\Psi_{N+1})^{-1}
\end{eqnarray*}
\end{frame}


%\end{CJK*}
\end{document}
