\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
%\begin{CJK*}{GBK}{song}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\def\lecturename{System Identification}

\title{\insertlecture}

\author{Xing Chao}

\institute
{
   School of Astronautics, Northwestern Polytechnical University
 
}

%\mode<presentation>{\subject{嵌入式系统}}

%  start a lecture  --------------------------
\lecture[LS]{Least squares identification}{}
\subtitle{For white noise}
\date{2017}


%\setbeamertemplate{background}{\pgfimage[width=\paperwidth,height=\paperheight]{image/flower}}
%\setbeamercovered{transparent}
%\mode<presentation>{\beamerdefaultoverlayspecification{<+->}}

\begin{frame}
  \maketitle
\end{frame}

\section{Least square estimation}

\begin{frame}{System model based on input/output data}
 The difference equation of a SISO system is
$$
y_k+a_1 y_{k-1}+\cdots + a_n y_{k-n}=b_0 u_k+\cdots + b_n u_{k-n}+\xi_k
$$
at time $k=n+1,n+2,\cdots,n+N$，there is
\begin{gather*}
y_{n+1}+a_1 y_n+\cdots + a_n y_1 = b_0 u_{n+1}+\cdots + b_n u_1+\xi_{n+1}  \\
y_{n+2}+a_1 y_{n+1}+\cdots + a_n y_2 = b_0 u_{n+2}+\cdots + b_n u_{2}+\xi_{n+2}  \\
\cdots    \\
y_{n+N}+a_1 y_{n+N-1}+\cdots + a_n y_{N} = b_0 u_{n+N}+\cdots + b_n u_{N}+\xi_{n+N}
\end{gather*}
\end{frame}

\begin{frame}{ Vector form }
\begin{eqnarray*}
Y &=& \Phi \theta + \xi \\
Y &=& \begin{bmatrix}  y_{n+1} & y_{n+2} & \cdots & y_{n+N}  \end{bmatrix}^T  \\
\Phi &=& \begin{bmatrix}
-y_{n}    & \cdots & -y_{1}   & u_{n+1} & \cdots & u_{1}  \\
-y_{n+1}  & \cdots & -y_{2}   & u_{n+2} & \cdots & u_{2}  \\
\vdots   &        & \vdots  & \vdots &        & \vdots \\
-y_{n+N-1} & \cdots & -y_{N}   & u_{n+N} & \cdots & u_{N}
\end{bmatrix} \\
\theta &=& \begin{bmatrix}a_1 & \cdots & a_n & b_0 & \cdots & b_n \end{bmatrix}^T\\
\xi &=& \begin{bmatrix}  \xi_{n+1} &  \xi_{n+2} & \cdots & \xi_{n+N} \end{bmatrix}^T
\end{eqnarray*}
\end{frame}

\begin{frame}{ Basic least squares method: identification criteria }
Identification criterion: least square sum of residuals.

%\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\begin{eqnarray*}
J &=& \sum_{k=n+1}^{n+N}e^2(k)  \\
&=& (Y-\Phi\hat\theta)^T(Y-\Phi\hat\theta) \\
\hat\theta_{LS} &=& \argmin_{\hat\theta} J
\end{eqnarray*}
\end{frame}

\begin{frame}{ Basic least square method: Derivative }
\begin{eqnarray*}
\frac{\partial J}{\partial \hat\theta_k} &=& \frac{\partial \sum_i(Y_i-\sum_m\Phi_{i,m}\hat\theta_m)^2}{\partial\hat\theta_k}  \\
&=& 2\sum_i(Y_i-\sum_m\Phi_{i,m}\hat\theta_m)\frac{\partial (Y_i-\sum_m\Phi_{i,m}\hat\theta_m)}{\partial\hat\theta_k} \\
&=& 2\sum_i(Y_i-\sum_m\Phi_{i,m}\hat\theta_m)\frac{\partial(-\sum_m\Phi_{i,m}\hat\theta_m)}{\partial\hat\theta_k} \\
&=& -2\sum_i(Y_i-\sum_m\Phi_{i,m}\hat\theta_m)\Phi_{i,k} \\
\frac{\partial J}{\partial \hat\theta}&=& (-2(Y-\Phi\hat\theta)^T\Phi)^T \\
&=& -2\Phi^T(Y-\Phi\hat\theta) 
\end{eqnarray*}
\end{frame}

\begin{frame}{ Basic least square method: solution }
\begin{eqnarray*}
-2\Phi^T(Y-\Phi\hat\theta_{LS}) &=&0\\
\Phi^T Y-\Phi^T\Phi\hat\theta_{LS} &=&0\\
\Phi^T Y &=& \Phi^T\Phi\hat\theta_{LS} \\
\hat\theta_{LS}&=& (\Phi^T\Phi)^{-1}\Phi^T Y 
\end{eqnarray*}
\end{frame}

\begin{frame}{ Basic least square method: two order derivative }
\begin{eqnarray*}
\frac{\partial^2 J}{\partial\hat\theta^2} &=& \frac{\partial (-2\Phi^T(Y-\Phi\hat\theta))}{\partial\hat\theta} \\
\frac{\partial \frac{\partial J}{\partial\hat\theta}}{\partial \hat\theta_s}&=& \frac{\partial (-2\sum_i(Y_i-\sum_m\Phi_{i,m}\hat\theta_m)\Phi_{i,k})}{\partial \hat\theta_s} \\
&=& 2\sum_i\frac{\partial \sum_m\Phi_{i,m}\hat\theta_m}{\partial \hat\theta_s}\Phi_{i,k} \\
&=& 2\sum_i\Phi_{i,s}\Phi_{i,k} \\
\frac{\partial^2 J}{\partial\hat\theta^2} &=& 2\Phi^T\Phi
\end{eqnarray*}
\end{frame}


\begin{frame}{The requirement of input signal by least square method : $\begin{bmatrix} Y_{N \times n} &  U_{N\times(n+1)}\end{bmatrix}$}
\begin{eqnarray*}
\Phi^T\Phi &=&\begin{bmatrix} Y_{N \times n} &  U_{N\times(n+1)}\end{bmatrix}^T\begin{bmatrix} Y_{N\times n} &  U_{N\times(n+1)}\end{bmatrix} \\
&=& \begin{bmatrix}
Y_{N \times n}^T Y_{N\times n}  & Y_{N \times n}^T U_{N\times(n+1)}  \\
U_{N\times(n+1)}^T Y_{N\times n}  &  U_{N\times(n+1)}^T U_{N\times(n+1)}
\end{bmatrix}
\end{eqnarray*}
where：
\begin{eqnarray*}
Y_{N\times n} &=& 
\begin{bmatrix}
-y_{n}    & \cdots & -y_{1}    \\
-y_{n+1}  & \cdots & -y_{2}    \\
\vdots    &        & \vdots   \\
y_{n+N-1} & \cdots & -y_{N}  
\end{bmatrix} \\
U_{N\times(n+1)} &=&
\begin{bmatrix}
u_{n+1} & \cdots & u_{1} \\
u_{n+2} & \cdots & u_{2} \\
\vdots &         & \vdots \\
u_{n+N} & \cdots & u_{N} 
\end{bmatrix} 
\end{eqnarray*}
\end{frame}

\begin{frame}{ The requirement of input signal by least square method $\begin{bmatrix} Y_{N \times n} &  U_{N\times(n+1)}\end{bmatrix}$}
\begin{eqnarray*}
(Y_{N\times n}^T Y_{N\times n})_{i,j} &=& \sum_{k=1}^{N-1+\min\{i,j\}}y_{n-i+k}y_{n-j+k} \\
(Y_{N\times n}^T U_{N\times (n+1)})_{i,j} &=& -\sum_{k=1}^{N-1+\min\{i,j-1\}}y_{n-i+k}u_{n+1-j+k} \\
(U_{N\times (n+1)}^T Y_{N\times n}^T )_{i,j} &=& -\sum_{k=1}^{N-1+\min\{j,i-1\}}y_{n-j+k}u_{n+1-i+k} \\
(U_{N\times (n+1)}^T U_{N\times (n+1)})_{i,j} &=& \sum_{k=1}^{N-2+\min\{i,j\}}u_{n+1-i+k}u_{n+1-j+k}
\end{eqnarray*}
\end{frame}

\begin{frame}{ The requirement of input signal by least square method $\begin{bmatrix}R_y & R_{yu} \\ R_{uy} & R_{u}\end{bmatrix}$}
\begin{eqnarray*}
\lim_{N\rightarrow\infty}\frac{\Phi^T\Phi}{N} &=& 
\frac{1}{N}
\begin{bmatrix}
Y_{N \times n}^T Y_{N\times n}  & Y_{N \times n}^T U_{N\times(n+1)}  \\
U_{N\times(n+1)}^T Y_{N\times n}  &  U_{N\times(n+1)}^T U_{N\times(n+1)} 
\end{bmatrix}\\
&=& \begin{bmatrix} R_y & R_{yu} \\ R_{uy} & R_u\end{bmatrix}
\end{eqnarray*}
where：
\begin{eqnarray*}
R_y &=& 
\begin{bmatrix}
R_y(0) & R_y(1) & \cdots & R_y(n-1) \\
R_y(1) & R_y(0) & \cdots & R_y(n-2) \\
\vdots & \vdots &        & \vdots   \\
R_y(n-1) & R_y(n-2) & \cdots & R_y(0) \\
\end{bmatrix}\\
R_{yu} &=&
\begin{bmatrix}
-R_{yu}(1) & -R_{yu}(0) & \cdots & -R_{yu}(1-n) \\
-R_{yu}(2) & -R_{yu}(1) & \cdots & -R_{yu}(2-n) \\
\vdots     & \vdots     &        &\vdots \\
-R_{yu}(n) & -R_{yu}(n-1) & \cdots & -R_{yu}(0) 
\end{bmatrix} \\
\end{eqnarray*}
\end{frame}

\begin{frame}{ The requirement of input signal by least square method: $\begin{bmatrix}R_y & R_{yu} \\ R_{uy} & R_{u}\end{bmatrix}$}
\begin{eqnarray*}
R_{uy} &=& R_{yu}^T \\
R_{uu} &=&
\begin{bmatrix}
R_u(0) & R_u(1) & \cdots & R_u(n) \\
R_u(1) & R_u(0) & \cdots & R_u(n-1) \\
\vdots     & \vdots     &        &\vdots \\
R_u(n) & R_u(n-1) & \cdots & R_u(0)
\end{bmatrix}
\end{eqnarray*}

%于是有：J取得极小值→φTφ正定→R正定→Ru正定。

%因此：J取得极小值的必要条件为Ru为正定阵。
\end{frame}

\begin{frame}{(n+1) order continuous excitation signal }
\begin{itemize}
\item defination：$\{u(k)\}$ is called $(n+1)$ order continuous excitation signal if  (n+1) order matrix $R_u$ of series $\{u(k)\}$ is positive definate，
。
\item The requirement of least square method for input signal is: $\{u(k)\}$ is $(n+1)$ order continuous excitation signal
\item $R_u$ is positive definate if $R_u$ is an strongly diagonally dominant matrix. The following sginals can satisfy the requirement of positive definate of $R_u$.
\begin{itemize}
\item  White noise sequence ；
\item  Pseudo random two bit noise sequence ；
\item  Colored noise random signal sequence 。
\end{itemize}
\item  "Pseudo random two bit noise sequence" and "colored noise random signal sequence" are often used as input signals in Engineering 。
\end{itemize}
\end{frame}

\mode<beamer>{
\begin{frame}{ Probabilistic properties of least squares estimation }
There are four main aspects of the probability property of least squares estimation,
\begin{itemize}
\item  unbiasedness of the estimation ；
\item  Consistency of estimates ；
\item  Validity of estimation ；
\item  Asymptotic normality of estimators .
\end{itemize}
 %We discuss the first two items: unbiased and consistent. 
\end{frame}
}

\begin{frame}{unbiasedness of the estimation}
$\hat\theta$ is refered as unbiased estimation of parameter $\theta$ if $E\{\hat\theta\}=\theta$ .
\begin{eqnarray*}
Y &=& \Phi\theta +\xi \\
\hat\theta &=& (\Phi^T\Phi)^{-1}\Phi^T Y \\
E[\hat\theta] &=& E[(\Phi^T\Phi)^{-1}\Phi^T Y] \\
&=& E[(\Phi^T\Phi)^{-1}\Phi^T (\Phi\theta+\xi)] \\
&=& E[(\Phi^T\Phi)^{-1}\Phi^T \Phi\theta+(\Phi^T\Phi)^{-1}\Phi^T \xi] \\
&=& E[\theta+(\Phi^T\Phi)^{-1}\Phi^T \xi] 
\end{eqnarray*}
 The necessary and sufficient conditions of least squares estimation for unbiased estimation is：
\begin{eqnarray*}
 E[(\Phi^T\Phi)^{-1}\Phi^T \xi] &=& 0
\end{eqnarray*}
%LS无偏估计的充分条件为：$\{\xi(k)\}$为零均值不相关随机序列，且与$\{u(k)\}$无关。
\end{frame}

\begin{frame}{ Consistent estimation }
\article{The estimated value is consistent if the estimated parameter converges to the true value $\theta$ in probability 1. defination：}
$$
\lim_{N\rightarrow\infty}P\{|\hat\theta-\theta\}=1
$$
Suppose $\xi\{(k)\}$ is random sequence with zero mean and independent distribution uncorrelated with $\{u(k)\}$:
\begin{eqnarray*}
 E(\hat\theta-\theta)^2 &=& E[(\Phi^T\Phi)^{-1}\Phi^T\xi\xi^T\Phi(\Phi^T\Phi)^{-1}] \\
 &=& E[\frac{1}{N^2}(\frac{1}{N}\Phi^T\Phi)^{-1}\Phi^T\xi\xi^T \Phi(\frac{1}{N}\Phi^T\Phi)^{-1}] \\
   \lim_{N\rightarrow\infty}E(\hat\theta-\theta)^2  &=& \frac{1}{N^2}R^{-1}E[\Phi^T\xi\xi^T\Phi] R^{-1} \\
  &=& \frac{1}{N^2}R^{-1}\sigma^2 E[\Phi^T\Phi] R^{-1} \\
  &=& \frac{1}{N^2}R^{-1}\sigma^2 N R R^{-1} \\
  &=& \frac{\sigma^2}{N}R^{-1} \\
  &=& 0
\end{eqnarray*}
\end{frame}

\mode<beamer>{
\begin{frame}{The validity of the estimation}
\article{ If the variance of the parameter estimation error reaches the minimum, then the estimated value is the valid estimation value。}
%若$\{\xi(k)\}$是零均值且服从正态分布的白噪声序列。则最小二乘参数估计值$\hat\theta$为有效估计值，参数估计偏差的方差达到Cramer-Rao不等式的下界
Cramer-Rao inequality：
$$
D\hat\theta = M^{-1}
$$
其中：
$$
M=E\left[\left(\frac{\partial\ln p(y|\theta)}{\partial\theta}\right)^T\left(\frac{\partial\ln p(y|\theta)}{\partial\theta}\right)\right]
$$
\end{frame}
}
\mode<beamer>{
\begin{frame}{The validity of the estimation}
\begin{eqnarray*}
y &=& \Phi\theta+\xi \\
y &\sim& N(\Phi\theta,\sigma^2 I) \\
p(y|\theta) &=& (2\pi\sigma^2)^{-\frac{N}{2}} exp\left[-\frac{1}{2\sigma^2}(y-\Phi\theta)^T(y-\Phi\theta)\right] \\
\frac{\partial \ln p(y|\theta)}{\partial \theta} &=& -\frac{1}{\sigma^2}(y-\Phi\theta)^T\Phi \\
M &=& E\left[\frac{1}{\sigma^4}\Phi^T(y-\Phi\theta)(y-\Phi\theta)^T\Phi \right]\\
 &=& \frac{1}{\sigma^4}E[\Phi^T \xi \xi^T \Phi ]\\
\lim_{N\to\infty}M^{-1} &=& \sigma^4(\sigma^2 E[\Phi^T \Phi ])^{-1}\\
&=& \frac{\sigma^2}{N}R^{-1} 
\end{eqnarray*}
\end{frame}
}
\mode<beamer>{
\begin{frame}{ Asymptotic normality of estimation}
Suppose $\{\xi(k)\}$ is  white noise sequence with zero mean and normal distribution. then：
\begin{eqnarray*}
y &=& \Phi\theta+\xi \\
y &\sim& N(\Phi\theta,\sigma^2 I) \\
\hat\theta &=& (\Phi^T\Phi)^{-1}\Phi^T y 
\end{eqnarray*}
\end{frame}
}

\section{ Model order increasing algorithm}
\begin{frame}{Model order increasing algorithm: algorithm characteristics}
\begin{itemize}
\item recursive algorithm based on model order $n$；
\item suitable for unknown model order $n$
\item The identification accuracy is the same as that of the basic least square method
\item  The identification speed is greatly improved than the basic least square method 
\item  It is not necessary to compute the inverse of higher order matrices 
\end{itemize}
\end{frame}

\begin{frame}{System model}
\begin{eqnarray*}
Y &=& \Phi_n\theta_n +\xi \\
\Phi_n &=& \begin{bmatrix}
u_{n+1}  &   -y_{n}    & u_{n}      & \cdots & -y_{1}  & u_{1}  \\
u_{n+2}  &   -y_{n+1}  & u_{n+1}    & \cdots & -y_{2}  & u_{2}  \\
\vdots   &   \vdots    & \vdots     &        & \vdots  & \vdots \\
u_{n+N}  &   -y_{n+N-1} & u_{n+N-1} & \cdots & -y_{N}  & u_{N}
\end{bmatrix} \\
&=& \begin{bmatrix}X_1 & \cdots & X_{2n+1} \end{bmatrix} \\
\theta_n &=& \begin{bmatrix}b_0 & a_1 & b_1 & \cdots & a_n & b_n \end{bmatrix}^T \\
\xi &=& \begin{bmatrix}\xi_{n+1} &\cdots & \xi_{n+N}\end{bmatrix}^T  \\
Y &=& \begin{bmatrix}y_{n+1} &\cdots & y_{n+N}\end{bmatrix}^T 
\end{eqnarray*}
\end{frame}

\begin{frame}{Identification from $n=0$}
\begin{eqnarray*}
\Phi_0 &=& X_1 \\
\hat\theta_0 &=& (\Phi_0^T\Phi_0)^{-1}\Phi_0^T Y \\
&=& \frac{\displaystyle \sum_{i=n+1}^{n+N}u_iy_i}{\displaystyle \sum_{i=n+1}^{n+N}u_i^2}
\end{eqnarray*}
\end{frame}

\begin{frame}{from $n$ to $n+1$}
Identification result of model order $n+1$ is obtained based on result of model order $n$。
 The solution is divided into two steps. First $\tilde P_n$ is solved, and then  $P_{n+1}$ is solved.
\begin{eqnarray*}
\Phi_{n+1} &=& \begin{bmatrix} \Phi_{n} & X_{2n+2} & X_{2n+3} \end{bmatrix} \\
 &=& \begin{bmatrix} \tilde\Phi_{n} & X_{2n+3} \end{bmatrix} \\
\tilde\Phi_n &\triangleq& \begin{bmatrix} \Phi_{n} & X_{2n+2} \end{bmatrix}  \\
P_n &\triangleq& (\Phi_{n}^T\Phi_{n})^{-1} \\
\tilde{P}_n &\triangleq& (\tilde\Phi_{n}^T\tilde\Phi_{n})^{-1}  \\
P_{n+1} &=& (\Phi_{n+1}^T\Phi_{n+1})^{-1} 
\end{eqnarray*}
\end{frame}

\begin{frame}{from $n$ to $n+1$：$P_{n+1}$}
\begin{eqnarray*}
P_{n+1} &=& 
\begin{bmatrix}
\tilde\Phi_n^T\tilde\Phi_n & \tilde\Phi_n^T X_{2n+3} \\
X_{2n+3}^T\tilde\Phi_n    & X_{2n+3}^T X_{2n+3}
\end{bmatrix}^{-1} \\
&=& \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \\
A_{22} &=& (X_{2n+3}^T X_{2n+3}-X_{2n+3}^T\tilde\Phi_n\tilde{P}_n\tilde\Phi_n^T X_{2n+3})^{-1} \\
A_{12} &=& A_{21}^T \\
&=& -\tilde{P}_n\tilde\Phi_n^T X_{2n+3} A_{22} \\
A_{11} &=& \tilde{P}_n- A_{12} X_{2n+3}^T\tilde\Phi_n \tilde{P}_n^T
\end{eqnarray*}
\end{frame}

\begin{frame}{from $n$ to $n+1$：$\tilde P_n$}
\begin{eqnarray*}
\tilde{P}_n &=& 
\begin{bmatrix}
\Phi_n^T\Phi_n & \Phi_n^T X_{2n+2} \\
X_{2n+2}^T\Phi_n    & X_{2n+2}^T X_{2n+3}
\end{bmatrix}^{-1} \\
&=& \begin{bmatrix} B_{11} & B_{12} \\ B_{21} & B_{22} \end{bmatrix} \\
B_{22} &=& (X_{2n+2}^T X_{2n+2}-X_{2n+2}^T\Phi_n P_n\Phi_n^T X_{2n+2})^{-1} \\
B_{12} &=& B_{21}^T \\
&=& -P_n \Phi_n^T X_{2n+2} B_{22} \\
B_{11} &=& P_n- B_{12} X_{2n+2}^T\Phi_n P_n^T
\end{eqnarray*}
\end{frame}


\begin{frame}{Computation procedure}
\begin{itemize}
\item  Initialize,compute $P_0=(\Phi_0^T\Phi_0)^{-1}$
\item  Compute $\hat\theta_0=P_0\Phi_0^T Y$
\item iterate
\begin{itemize}
\item  Compute $\tilde{P}_n$ based on $P_n$
\item  Compute $P_{n+1}$ based on $\tilde{P}_n$
\item  Compute $\hat\theta_{n+1}=P_{n+1}\Phi_{n+1}^T Y$
\end{itemize}
\end{itemize}
\end{frame}

\section{recursive least square}
%\article{解决问题：(n+N)组观测数据时的参数估计值已知，现在又得到了一组新的观测值(u(n+N+1),y(n+N+1))，如何采用最小二乘法进行在线估计新的估计值问题}

\begin{frame}{ Recursive algorithm derivation: Model }
Assuming that the input and output data with length of $N$ have been obtained, the least squares estimation is 
\begin{eqnarray*}
Y_N &=& \Phi_N\theta+\xi_N \\
\hat\theta_N &=& (\Phi_N^T\Phi_N)^{-1}\Phi_N^T Y_N \\
\tilde\theta_N &=& \theta-\tilde\theta_N \\
&=& -(\Phi_N^T\Phi_N)^{-1}\Phi_N^T\xi_N
\end{eqnarray*}
After obtaining new data $u_{n+N+1},y_{n+N+1}$,
\begin{eqnarray*}
y_{(n+N+1)} &=& \Psi^T\theta+\xi_{(n+N+1)} \\
y_{N+1} &=& \Psi^T\theta+\xi_{N+1} \\
\Psi_i &=& \begin{bmatrix} -y_{(n+i-1)} & \cdots & -y_{(i)} & u_{(n+i)} & \cdots & u_{(i)} \end{bmatrix}^T  \\
\begin{bmatrix} Y_N \\ y_{N+1} \end{bmatrix} &=& 
\begin{bmatrix} \Phi_N \\ \Psi_{N+1}^T \end{bmatrix} \theta +
\begin{bmatrix} \xi_N \\ \xi_{N+1} \end{bmatrix} 
\end{eqnarray*}
\end{frame}

\bgroup
\setbeamertemplate{sidebar right}{}
\begin{frame}{ Recursive algorithm derivation :$P_{N+1}$}
\begin{eqnarray*}
\hat\theta_{N+1} &=&
\left(
\begin{bmatrix} \Phi_N \\ \Psi_{N+1}^T\end{bmatrix}^T
\begin{bmatrix} \Phi_N \\ \Psi_{N+1}^T\end{bmatrix}
\right)^{-1}
\begin{bmatrix} \Phi_N \\ \Psi_{N+1}^T\end{bmatrix}^T
\begin{bmatrix} Y_N \\ y_{N+1}\end{bmatrix} \\
&=& (\Phi_N^T\underbrace{\Phi_N}_{N,2n+1}+\underbrace{\Psi_{N+1}}_{2n+1,1}\Psi_{N+1}^T)^{-1}(\Phi_N^T \underbrace{Y_N}_{N,1} +\Psi_{N+1}\underbrace{y_{N+1}}_{1,1})  \\
\hat\theta_{N+1} &=& P_{N+1}(\Phi_N^T Y_N + \Psi_{N+1} y_{N+1}) 
\end{eqnarray*}
其中：
\begin{eqnarray*}
P_{N+1} &=&(P_N^{-1}+\Psi_{N+1}\Psi_{N+1}^T)^{-1} \\
P_{N} &=& (\Phi_N^T\Phi_N)^{-1}
\end{eqnarray*}
\end{frame}
\egroup

\begin{frame}{ Recursive algorithm derivation : matrix inversion lemma}
If the inverse of the corresponding matrix exists ，then:
$$
(A+BC^T)^{-1}=A^{-1}-A^{-1}B(I+C^T A^{-1} B)^{-1} C^T A^{-1}
$$
therefore：
\begin{eqnarray*}
P_{N+1} &=&(P_N^{-1}+\Psi_{N+1}\Psi_{N+1}^T)^{-1} \\
&=& P_N -P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}\Psi_{N+1}^T P_N \\
\hat\theta_{N+1} &=& A+B \\
A&=&P_{N+1}\Phi_N^T Y_N \\
B&=&P_{N+1}\Psi_{N+1} y_{N+1} \\
i&=& 1+\Psi_{N+1}^T P_N \Psi_{N+1}
\end{eqnarray*}
\end{frame}

\bgroup
\setbeamertemplate{sidebar right}{}
\begin{frame}{Recursive algorithm derivation:Simplification}
\begin{eqnarray*}
A &=& (P_N -P_N\Psi_{N+1}i^{-1}\Psi_{N+1}^T P_N )\Phi_N^T Y_N\\
&=& P_N\Phi_N^T Y_N -P_N\Psi_{N+1}i^{-1}\Psi_{N+1}^T P_N \Phi_N^T Y_N\\
&=& \hat\theta_N-P_N\Psi_{N+1}i^{-1}\Psi_{N+1}^T \hat\theta_N\\
B &=& (P_N -P_N\Psi_{N+1}i^{-1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
&=& i^{-1}(P_N(1+\Psi_{N+1}^T P_N \Psi_{N+1}) -P_N\Psi_{N+1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
&=& i^{-1}(P_N+P_N\Psi_{N+1}^T P_N \Psi_{N+1} -P_N\Psi_{N+1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
&=& i^{-1}(P_N\Psi_{N+1}+P_N\Psi_{N+1}^T P_N \Psi_{N+1}\Psi_{N+1} \\
&&  -P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1})y_{N+1}\\
&=& i^{-1}(P_N\Psi_{N+1}+P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1} \\
&& -P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1})y_{N+1}\\
&=& i^{-1}P_N\Psi_{N+1}y_{N+1}
% A &=& (P_N -P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}\Psi_{N+1}^T P_N )\Phi_N^T Y_N\\
% &=& P_N\Phi_N^T Y_N -P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}\Psi_{N+1}^T P_N \Phi_N^T Y_N\\
% &=& \hat\theta_N-P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}\Psi_{N+1}^T \hat\theta_N\\
% B &=& (P_N -P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
%  &=& (1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}(P_N(1+\Psi_{N+1}^T P_N \Psi_{N+1}) -P_N\Psi_{N+1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
%  &=& (1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}(P_N+P_N\Psi_{N+1}^T P_N \Psi_{N+1} -P_N\Psi_{N+1}\Psi_{N+1}^T P_N )\Psi_{N+1}y_{N+1}\\
%  &=& (1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}(P_N\Psi_{N+1}y_{N+1}+P_N\Psi_{N+1}^T P_N \Psi_{N+1}\Psi_{N+1}y_{N+1} -P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1}y_{N+1})\\
%  &=& (1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}(P_N\Psi_{N+1}+P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1} -P_N\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1})y_{N+1}\\
%  &=& (1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1}P_N\Psi_{N+1}y_{N+1}\\
\end{eqnarray*}
\article{note：$\Psi_{N+1}^T P_N \Psi_{N+1}$ is a scalar}
\end{frame}
\egroup

\begin{frame}{Recursive algorithm derivation：result}
\begin{eqnarray*}
\hat\theta_{N+1} &=& \hat\theta_N-P_N\Psi_{N+1}i^{-1}\Psi_{N+1}^T \hat\theta_N +i^{-1}P_N\Psi_{N+1}y_{N+1} \\
&=& \hat\theta_N+i^{-1}P_N\Psi_{N+1}(-\Psi_{N+1}^T \hat\theta_N +y_{N+1}) \\
&=& \hat\theta_N+ K_{N+1}(y_{N+1}-\Psi_{N+1}^T\hat\theta_N) \\
K_{N+1} &=& P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi_{N+1})^{-1} \\
P_{N+1} &=& P_N -K_{N+1}\Psi_{N+1}^T P_N 
\end{eqnarray*}

Obtain initial value:
\begin{itemize}
\item  Basic least squares estimation 
\item $\hat\theta_0=0,P_0=c^2I$, where $c$ is a sufficient large constant。
\end{itemize}
\end{frame}

\begin{frame}{Convergence:$P_N$}
\begin{eqnarray*}
P_N &=& (P_{N-1}^{-1}+\Psi_N\Psi_N^T)^{-1} \\
P_N^{-1} &=& P_{N-1}^{-1}+\Psi_N\Psi_N^T  \\
P_{N-1}^{-1} &=& P_{N-2}^{-1}+\Psi_{N-1}\Psi_{N-1}^T  \\
P_{N-2}^{-1} &=& P_{N-3}^{-1}+\Psi_{N-2}\Psi_{N-2}^T  \\
P_{N-3}^{-1} &=& P_{N-4}^{-1}+\Psi_{N-3}\Psi_{N-3}^T  \\
& \vdots & \\
P_1^{-1} &=& P_0^{-1}+\Psi_1\Psi_1^T  \\
P_N^{-1} &=& P_0^{-1}+\sum_{i=1}^{N}\Psi_i\Psi_i^T \\
\end{eqnarray*}
\end{frame}

\begin{frame}{Convergence}
\article{$\Psi_{i}$ is corresponding to the i'th row of $\Phi_{N}$}
\begin{eqnarray*}
\Phi_N &=& \begin{bmatrix} \Psi_{1}^T \\ \Psi_{2}^T \\ \vdots \\ \Psi_{N}^T \end{bmatrix}  \\
P_N^{-1} &=& \frac{1}{c^2}I+
\begin{bmatrix} \Psi_{1} & \Psi_{2} & \cdots & \Psi_{N} \end{bmatrix}
\begin{bmatrix} \Psi_{1}^T \\ \Psi_{2}^T \\ \vdots \\ \Psi_{N}^T \end{bmatrix}  \\
&=& \frac{1}{c^2}I+\Phi^T\Phi \\
\lim_{c\to\infty}P_N^{-1} &=& \Phi_N^T\Phi_N  \\
\hat\theta_N &=& P_N\Phi_N^T Y_N \\
 &=& (\Phi_N^T\Phi_N)^{-1}\Phi_N^T Y_N 
\end{eqnarray*}
\end{frame}

\section{Problem discussion}
\begin{frame}{ The relationship between residual and innovation }
Innovation $\tilde y_i = y_i - \Psi_i^T \hat\theta_{i-1}$ is used to describe prediction error at time $i$.
residual $\varepsilon_i = y_i - \Psi_i^T\hat\theta_i$ is used to describe the output bias at time $i$.
\begin{eqnarray*}
\varepsilon &=& y_i - \Psi_i^T\hat\theta_i  \\
&=& y_i - \Psi_i^T(\hat\theta_{i-1}+K_i \tilde y_i) \\
&=& \tilde y_i -\Psi_i^T K_i \tilde y_i \\
&=& (1-\Psi_i^T K_i) \tilde y_i \\
&=& (1-\Psi_i^T P_{i-1} \Psi_i(\Psi_i^T P_{i-1} \Psi_i+1)^{-1}) \tilde y_i \\
&=& \frac{\Psi_i^T P_{i-1} \Psi_i+1-\Psi_i^T P_{i-1} \Psi_i}{\Psi_i^T P_{i-1} \Psi_i+1} \tilde y_i \\
&=& \frac{\tilde y_i}{\Psi_i^T P_{i-1} \Psi_i+1}  
\end{eqnarray*}
\end{frame}

\begin{frame}{ Recursive calculation of criterion function }
\begin{eqnarray*}
J_{i} &=& (Y_i-\Phi_i\theta_i)^T (Y_i-\Phi_i\theta_i)   \\
J_{i-1} &=& (Y_{i-1}-\Phi_{i-1}\theta_{i-1})^T (Y_{i-1}-\Phi_{i-1}\theta_{i-1}) \\
Y_i-\Phi_i\theta_i &=& Y_i - \Phi_i (\hat\theta_{i-1}+K_i \tilde y_i) \\
&=& \begin{bmatrix}  Y_{i-1}  \\  y_i \end{bmatrix} -\begin{bmatrix}\Phi_{i-1} \\ \Psi_i^T \end{bmatrix} (\hat\theta_{i-1}+ K_i \tilde y_k)  \\
&=& \begin{bmatrix} Y_{i-1}-\Phi_{i-1}\hat\theta_{i-1}  \\ \tilde y_i \end{bmatrix} -\begin{bmatrix}\Phi_{i-1} \\ \Psi_i^T \end{bmatrix}  K_i \tilde y_k  \\
\end{eqnarray*}
\end{frame}

\begin{frame}{ Recursive calculation of criterion function }
\begin{eqnarray*}
J_i &=& J_{i-1} - 2 K_i^T\Phi_{i-1}^T(Y_{i-1}-\Phi_{i-1}\hat\theta_{i-1})\tilde y_i + K_i^T \Phi_{i-1}^T\Phi_{i-1} K_i \tilde y_i^2  \\
&& +(1-2K_i^T\Psi_i+K_i^T\Psi_i\Psi_i^T K_i)\tilde y_i^2   \\
&=& J_{i-1} - 2 K_i^T(\Phi_{i-1}^TY_{i-1}-\Phi_{i-1}^T\Phi_{i-1}\hat\theta_{i-1})\tilde y_i  \\
&&  +(1-2K_i^T\Psi_i+K_i^T\Phi_i\Phi_i^T K_i)\tilde y_i^2   \\
&=& J_{i-1}  +  (1-2K_i^T\Psi_i+K_i^T\Phi_i\Phi_i^T K_i)\tilde y_i^2   \\
&=& J_{i-1}  +  (1-2K_i^T\Psi_i+K_i^T P_i^{-1} K_i)\tilde y_i^2   \\
&=& J_{i-1}  +  (1-2K_i^T\Psi_i+K_i^T \Psi_i)\tilde y_i^2   \\
&=& J_{i-1}  +  (1-K_i^T\Psi_i)\tilde y_i^2   \\
&=& J_{i-1}  +  (1-\Psi_i^T P_{i-1} \Psi_i(\Psi_i^T P_{i-1} \Psi_i+1)^{-1}) \tilde y_i^2 \\
&=& J_{i-1}  +  \frac{\Psi_i^T P_{i-1} \Psi_i+1-\Psi_i^T P_{i-1} \Psi_i}{\Psi_i^T P_{i-1} \Psi_i+1} \tilde y_i^2 \\
&=& J_{i-1}  +  \frac{\tilde y_i^2}{\Psi_i^T P_{i-1} \Psi_i+1}  
\end{eqnarray*}
\end{frame}
\begin{frame}{he influencTe of the calculation error of gain matrix $K_i$ on $P_i$}
When there is error $\delta K_i$ in $K_i$：
\begin{eqnarray*}
\delta P_i &=& \delta K_i \Psi_i^T P_{i-1}
\end{eqnarray*}
Compute new form of $P_i$：
\begin{eqnarray*}
P_i &=& (I-K_i\Psi_i^T)P_{i-1}  \\
&=& (I-K_i\Psi_i^T)P_{i-1} -P_{i-1}\Psi_i K_i^T+P_{i-1}\Psi_i K_i^T \\
&=& (I-K_i\Psi_i^T)P_{i-1} -P_{i-1}\Psi_i K_i^T+K_i(\Psi_i^T P_{i-1}\Psi_i+1)K_i^T \\
&=& (I-K_i\Psi_i^T)P_{i-1} -(I-K_i\Psi_i^T)P_{i-1}\Psi_i K_i^T+K_i K_i^T \\
&=& (I-K_i\Psi_i^T)(P_{i-1} -P_{i-1}\Psi_i K_i^T)+K_i K_i^T \\
&=& (I-K_i\Psi_i^T)P_{i-1} (I-\Psi_i K_i^T)+K_i K_i^T 
\end{eqnarray*}
\end{frame}

\begin{frame}{he influencTe of the calculation error of gain matrix $K_i$ on $P_i$}
When there is error $\delta K_i$ in $K_i$：
\begin{eqnarray*}
\delta P_i  &=& (I-(K_i+\delta K_i)\Psi_i^T)P_{i-1} (I-\Psi_i (K_i+\delta K_i)^T) \\
&& +(K_i+\delta K_i) (K_i+\delta K_i)^T -P_i  \\
 &=& -\delta K_i\Psi_i^T P_{i-1} (I-\Psi_i K_i^T)+K_i \delta K_i^T  \\
 && -(I-K_i\Psi_i^T)P_{i-1} \Psi_i \delta K_i^T+\delta K_i K_i^T   \\
 && + \delta K_i\Psi_i^T P_{i-1} \Psi_i \delta K_i^T+\delta K_i \delta K_i^T  \\
 && +(I-K_i\Psi_i^T)P_{i-1} (I-\Psi_i K_i^T)+K_i K_i^T -P_i  \\
 &=& -\delta K_i\Psi_i^T P_{i-1} (I-\Psi_i K_i^T)+K_i \delta K_i^T  \\
 && -(I-K_i\Psi_i^T)P_{i-1} \Psi_i \delta K_i^T+\delta K_i K_i^T  +O(\delta K_i) \\
 &=& -\delta K_i\Psi_i^T P_i^T+\delta K_i K_i^T -P_i \Psi_i \delta K_i^T+ K_i \delta K_i^T +O(\delta K_i) \\
 &=& -\delta K_i K_i^T +\delta K_i K_i^T -K_i \delta K_i^T+ K_i \delta K_i^T +O(\delta K_i) \\
 &=& O(\delta K_i) 
\end{eqnarray*}
\end{frame}

\begin{frame}{ Stability of recursive algorithms: Difference Equations }
\begin{eqnarray*}
y_i &=& \Psi_i^T \theta+\xi_i \\
\tilde\theta_i &\stackrel{def}{=}&\theta-\hat\theta_i \\
&=& \theta-[\hat\theta_{i-1} + K_i (y_i-\Psi_i^T\hat\theta_{i-1})] \\
&=& \tilde\theta_{i-1}-K_i (y_i-\Psi_i^T\hat\theta_{i-1}) \\
&=& \tilde\theta_{i-1}-K_i (\Psi_i^T\theta + \xi_i -\Psi_i^T\hat\theta_{i-1}) \\
&=& \tilde\theta_{i-1}-K_i (\Psi_i^T\tilde\theta_{i-1} + \xi_i ) \\
&=& (I-K_i \Psi_i^T)\tilde\theta_{i-1} - K_i\xi_i \\
&=& P_i P_{i-1}^{-1}\tilde\theta_{i-1} - K_i\xi_i \\
&=& A_i \tilde\theta_{i-1} - K_i\xi_i \\
A_i &=& P_i P_{i-1}^{-1} 
\end{eqnarray*}
\end{frame}

\begin{frame}{ Stability of recursive algorithms: eigenvalues }
\begin{eqnarray*}
A_i x &=& \lambda x \\
(P_{i-1}^{-1}+\Psi_i\Psi_i^T)^{-1} P_{i-1}^{-1}x &=& \lambda x \\
P_{i-1}^{-1}x &=& [P_{i-1}^{-1}+\Psi_i\Psi_i^T]\lambda x \\
(1-\lambda)P_{i-1}^{-1}x &=& \lambda \Psi_i\Psi_i^T x \\
(1-\lambda) x^T P_{i-1}^{-1}x &=& \lambda x^T \Psi_i\Psi_i^T x 
\end{eqnarray*}
where：$ P_{i-1}^{-1} $ is positive definite and $\Psi_i\Psi_i^T$ is non-negtive definite ， so $0<\lambda \leq 1$。that is ：$\tilde\theta_i\leq\tilde\theta_0$。
\end{frame}

\begin{frame}{ The relationship between least squares estimation and Kalman filtering }
State space model：
\begin{eqnarray*}
\theta_{i+1} &=& \theta_i \\
y_i &=& \Psi_i^T\theta_i +\xi_i
\end{eqnarray*}
Kalman filtering：
\begin{eqnarray*}
\hat\theta_i &=& \hat\theta_{i-1} +K_i(y_i-\Psi_i^T\hat\theta_{i-1})  \\
K_i &=& S_i\Psi_i(\Psi_i^T S_i\Psi_i+\sigma^2)^{-1}  \\
S_i &=&  P_{i-1} \\
P_i &=&   (I-K_i\Psi_i^T)P_{i-1} \\
\hat\theta_0 &=& 0 \\
\end{eqnarray*}
\end{frame}

%\begin{frame}{作业}
%\begin{itemize}
%\item 复习自相关函数、互相关函数概念
%\item 推导基本最小二乘辨识算法
%\item 推导可辨识条件
%\item 推导课本中关于最小二乘概率性质的结论
%\end{itemize}
%\end{frame}
%\end{CJK*}
\end{document}
