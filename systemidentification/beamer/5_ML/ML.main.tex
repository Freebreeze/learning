\begin{document}
%\begin{CJK*}{GBK}{song}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\def\lecturename{系统辨识}

\title{\insertlecture}

\author{邢超}

\institute
{
  西北工业大学航天学院
}

%\mode<presentation>{\subject{嵌入式系统}}

%  start a lecture  --------------------------
\lecture[MLE]{极大似然法辨识}{}
\subtitle{}
\date{2012}


%\setbeamertemplate{background}{\pgfimage[width=\paperwidth,height=\paperheight]{image/flower}}
%\setbeamercovered{transparent}
%\mode<presentation>{\beamerdefaultoverlayspecification{<+->}}

\begin{frame}
  \maketitle
\end{frame}


\section{简介}

\begin{frame}{基本思想}
\begin{description}
\item[辨识准则] 观测值的出现概率最大
\item[似然函数] 观察值的概率密度函数
\end{description}
\end{frame}

\begin{frame}{方法特点}
\begin{enumerate}
%\item 无偏估计方法；
\item 适用于$\xi(k)$相关情况；
\item 当系统信噪比较小时有较好的估计效果；
\item 算法稳定度好；
\item 有递推算法；
\item 实际工程中广泛使用
\end{enumerate}
\end{frame}

\section{极大似然原理}

\begin{frame}{似然函数}
 设某离散随机过程$\{V_k\}$与待辨识参数 $\theta$ 有关，其概率分布密度 $f(V_k|\theta)$ 已知，若测得$n$个独立的观测值 $V_1,V_2,\cdots,V_n$ ，其分布密度为： $f(V_1|\theta),\cdots,f(V_n|\theta)$ ，定义似然函数$L$为：
$$L(V_1,\cdots,V_n|\theta)=f(V_1|\theta)\cdot f(V_2|\theta)\cdots f(V_n|\theta)$$
\end{frame}

\begin{frame}{极大似然估计}
辨识$\theta$的原则就是使得$L$达到极大值，即:
$$\frac{\partial L}{\partial \theta}=0$$
通常对$L$取对数：
$$\ln L = \ln f(V_1|\theta)+\cdots + \ln f(V_n|\theta)$$
求解：
$$\frac{\partial \ln L}{\partial \theta}$$
所得$\theta$即为极大似然估计$\hat\theta_{ML}$
\end{frame}

\section{极大似然辨识}
\subsection{白噪声情况}
\begin{frame}{差分方程的极大似然辨识：系统模型（白噪声情况）}
系统差分方程：
$$a(z^{-1})y(k)=b(z^{-1})u(k)+\xi(k)$$
式中，$\xi(k)$为高斯白噪声序列且与$u(k)$无关。以向量形式表示：
$$\vect{Y_N}=\Phi_N\vect{\theta}+\vect{\xi}$$
\end{frame}

\begin{frame}{差分方程的极大似然辨识：残差（白噪声情况）}
系统估计残差：
\begin{eqnarray*}
\vect{e_N} &=& \vect{Y_N}-\Phi_N\vect{\hat\theta} \\
\vect{e_N} &=& [e(n+1),  e(n+2), \cdots, e(n+N)]^T
\end{eqnarray*}
由于$\xi(k)$为高斯白噪声，故假设$e(k)$也为高斯白噪声。
设$e(k)$方差为$\sigma^2$。概率密度函数为：
$$p(e(k)|\hat\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{e^2(k)}{2\sigma^2}}$$
\end{frame}

\begin{frame}{差分方程的极大似然辨识：似然函数（白噪声情况）}
似然函数为：
\begin{eqnarray*}
L(Y_N|\hat\theta)&=&\prod_{k=n+1}^{n+N}p(e(k)|\hat\theta) \\
&=&\frac{1}{(2\pi\sigma^2)^{N/2}}exp[-\frac{\sum e^2(k)}{2\sigma^2}] \\
&=&\frac{1}{(2\pi\sigma^2)^{N/2}}exp[-\frac{(Y_N-\Phi_N\hat\theta)^T(Y_N-\Phi_N\hat\theta)}{2\sigma^2}] \\
\ln L(Y_N|\hat\theta)&=& -\frac{N}{2}\ln 2\pi-\frac{N}{2}\ln\sigma^2-\frac{(Y_N-\Phi_N\hat\theta)^T(Y_N-\Phi_N\hat\theta)}{2\sigma^2}
\end{eqnarray*}
\end{frame}

\begin{frame}{差分方程的极大似然辨识：似然函数（白噪声情况）}
则依极大似然辨识原理有：
\begin{eqnarray*}
\frac{\partial\ln L(Y_N|\hat{\theta})}{\partial\hat{\theta}} &=& \frac{\Phi^T_N\vect{Y}_N-\Phi^T_N\Phi_N\hat{\vect{\theta}}}{\sigma^2}=0  \\
\frac{\partial\ln L(Y_N|\hat{\theta})}{\partial\hat{\sigma^2}} &=&-\frac{N}{2\sigma^2}+\frac{(\vect{Y}_N-\Phi_N\hat{\vect{\theta}})^T(\vect{Y}_N-\Phi_N\hat{\vect{\theta}})}{2\sigma^4}=0
\end{eqnarray*}
解上述方程有：
\begin{eqnarray*}
\vect{\hat\theta_{ML}} &=& (\vect\Phi^T_N\vect\Phi_N)^{-1}\vect\Phi^T_N\vect{Y}_N   \\
\sigma^2 &=& \frac{(\vect{Y}_N-\Phi_N\hat{\vect{\theta}})^T(\vect{Y}_N-\Phi_N\hat{\vect{\theta}})}{N}
\end{eqnarray*}
可见在$\xi(k)$为高斯白噪声序列这一特殊情况下，极大似然辨识与一般最小二乘法辨识具有相同结果。
\end{frame}

\subsection{有色噪声情况}

\begin{frame}{差分方程的极大似然辨识：系统模型（有色噪声情况）}
$$a(z^{-1})y(k) = b (z^{-1})u(k)+c(z^{-1})\varepsilon(k)$$
其中：
\begin{eqnarray*}
a(z^{-1}) &=&  1+a_1z^{-1}+ \cdots +a_n z^{-n} \\
b(z^{-1}) &=&  b_0+b_1z^{-1}+ \cdots +b_n z^{-n} \\
c(z^{-1}) &=&  1+c_1z^{-1}+ \cdots +c_n z^{-n} 
\end{eqnarray*}
\end{frame}

\bgroup
\setbeamertemplate{sidebar right}{}
\begin{frame}{差分方程的极大似然辨识：预测误差（有色噪声情况）}
预测误差：
$$
e(k)=y(k)-\hat y(k)
$$
其向量形式为：
$$
e_N=\vect{Y}_N-\Phi_N\hat\theta
$$
其中：
\begin{eqnarray*}
\vect{\hat\theta} &=& [\hat a_1, \cdots, \hat a_n, \hat b_0 , \cdots, \hat b_n, \hat c_1, \cdots, \hat c_n]^T \\
\vect{Y}_N &=& [y(n+1), \cdots, y(n+N)]^T \\
\vect{e}_N &=& [e(n+1), \cdots, e(n+N)]^T \\
\Phi_N &=& 
%\begin{bmatrix}
%-y(n) & \cdots & -y(1) & u(n+1) & \cdots & u(1)  & e(n) & \cdots & e(1) \\
%-y(n+1) & \cdots & -y(2) & u(n+2) & \cdots & u(2)  & e(n+1) & \cdots & e(2) \\
%\vdots &        & \vdots &   \vdots &      &\vdots & \vdots & \vdots & \vdots \\
%-y(n+N-1) & \cdots & -y(N) & u(n+N) & \cdots & u(N)  & e(n+N-1) & \cdots & e(N)
%\end{bmatrix}
\begin{bmatrix}
-y_n & \cdots & -y_1 & u_{n+1} & \cdots & u_1  & e_n & \cdots & e_1 \\
-y_{n+1} & \cdots & -y_2 & u_{n+2} & \cdots & u_2  & e_{n+1} & \cdots & e_2 \\
\vdots &        & \vdots &   \vdots &      &\vdots & \vdots & \vdots & \vdots \\
-y_{n+N-1} & \cdots & -y_N & u_{n+N} & \cdots & u_N  & e_{n+N-1} & \cdots & e_N
\end{bmatrix}
\end{eqnarray*}
\end{frame}
\egroup

\begin{frame}{差分方程的极大似然辨识：似然函数（有色噪声情况）}
因为$\varepsilon(k)$为高斯白噪声，故而$e(k)$可假设为零均值的高斯白噪声。则似然函数为：
\begin{eqnarray*}
L(Y_N|\hat{\theta}) &=& \frac{1}{(2\pi\sigma^2)^{N/2}}exp[-\frac{(\vect{Y}_N-\phi_N\hat{\vect\theta})^T(\vect{Y}_N-\phi_N\hat{\vect\theta})}{2\sigma^2}] \\
\ln L(Y_N|\hat{\theta}) &=& - \frac{N}{2}\ln{2\pi}-\frac{N}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^{n+N}_{k=n+1}e^2(k)
\end{eqnarray*}
由$\frac{\partial\ln L(Y_N|\hat{\theta})}{\partial\sigma^2}=0$得：
$$
\hat\sigma^2=\frac{1}{N}\sum^{n+N}_{k=n+1}e^2(k)
$$
\end{frame}

\begin{frame}{差分方程的极大似然辨识：准则（有色噪声情况）}
\begin{eqnarray*}
J &=& \frac{1}{2}\sum_{k=n+1}^{n+N}e^2(k)  \\
\sigma^2 &=& \frac{2}{N}J  \\
\ln L(Y_N|\hat{\theta}) &=& - \frac{N}{2}\ln{2\pi}-\frac{N}{2}\ln(\frac{2J}{N})-\frac{N}{2}
\end{eqnarray*}
\begin{itemize}
\item $J$为参数$a_1,a_2,\cdots,a_n;b_0,b_1,\cdots,b_n;c_1,c_2,\cdots,c_n $的二次型函数。
\item 使$L$最大的$\hat\theta$,等价于在约束条件
$$
\hat c(z^{-1})e(k)=\hat a(z^{-1})y(k)-\hat b(z^{-1})u(k)
$$
下求$\hat\theta$，使$J$最小。
\end{itemize}
\end{frame}

\begin{frame}{差分方程的极大似然辨识：牛顿-拉卜森法}
牛顿-拉卜森法的迭代公式：
$$
\hat\theta_1=\hat\theta_0-\left.\left[\left(\frac{\partial^2J}{\partial\theta^2}\right)^{-1}\frac{\partial J}{\partial\theta}\right]\right|_{\theta=\hat\theta_0}
$$
其中：
\begin{itemize}
\item $\frac{\partial J}{\partial\theta}$为梯度
\item $\frac{\partial^2J}{\partial\theta^2}$为海赛矩阵
\end{itemize}
\end{frame}


\begin{frame}{Newton-Raphson迭代步骤：初始值选定}
$$
\hat\theta_0=[\hat a_1,\cdots,\hat a_n,\hat b_0,\cdots,\hat b_n,\hat c_1,\cdots,\hat c_n]^T
$$
其中：
\begin{itemize}
\item  $\hat a_1,\cdots,\hat a_n,\hat b_0,\cdots,\hat b_n$可用最小二乘法求得
\item  $\hat c_1,\cdots,\hat c_n$可取为零或任意假定某一组值
\end{itemize}
\end{frame}

\begin{frame}{Newton-Raphson迭代步骤：计算预测误差}
\begin{itemize}
\item   预测误差，指标函数与误差方差估计值：
\begin{eqnarray*}
e(k) &=& y(k)-\hat y(k)   \\
J &=& \frac{\sum_{k=n+1}^{n+N}e^2(k)}{2}    \\
\sigma^2 &=& \frac{2J}{N}
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}{Newton-Raphson迭代步骤：计算梯度矩阵及海赛矩阵}
\begin{eqnarray*}
\frac{\partial J}{\partial\theta} &=& \sum_{k=n+1}^{n+N}e(k)\frac{\partial e(k)}{\partial\theta}  \\
\frac{\partial^2 J}{\partial\theta^2} &=& \sum_{k=n+1}^{n+N}\frac{\partial e(k)}{\partial\theta}\left[\frac{\partial e(k)}{\partial\theta}\right]^T+\sum_{k=n+1}^{n+N}e(k)\frac{\partial^2 e(k)}{\partial\theta^2}  \\
 &\approx& \sum_{k=n+1}^{n+N}\frac{\partial e(k)}{\partial\theta}\left[\frac{\partial e(k)}{\partial\theta}\right]^T
\end{eqnarray*}
其中：
$$
\frac{\partial e(k)}{\partial\theta}=\left[\frac{\partial e(k)}{\partial a_1},\cdots,\frac{\partial e(k)}{\partial a_n},\frac{\partial e(k)}{\partial b_0},\cdots,\frac{\partial e(k)}{\partial b_n},\frac{\partial e(k)}{\partial c_1},\cdots,\frac{\partial e(k)}{\partial c_n}\right]^T
$$
\end{frame}

\begin{frame}{牛顿-拉卜森迭代步骤：计算新的估计值}
$$
\hat{\vect\theta}_1 = \hat{\vect\theta}_0 - \left.\left[ \left(\frac{\partial^2 J}{\partial\vect\theta^2}\right)^{-1}\frac{\partial J}{\partial\vect\theta}\right]\right|_{\vect\theta=\hat{\vect\theta}_0}
$$
停上条件：
$$
\frac{\hat\sigma^2_{r+1}-\hat\sigma^2_r}{\hat\sigma^2_r}< \delta
$$
其中$\delta$可取较小的数，如$\delta=10^{-4}$。
\end{frame}
\section{递推极大似然法}
\subsection{近似的递推极大似然法}
\begin{frame}{系统差分方程}
$$
a(z^{-1})y(k) = b(z^{-1})u(k)+c(z^{-1})\varepsilon(k)
$$
其中：
\begin{eqnarray*}
a(z^{-1}) &=&  1+a_1 z^{-1}+\cdots+a_nz^{-n} \\
b(z^{-1}) &=&  b_0+b_1 z^{-1}+\cdots+b_n z^{-n} \\
c(z^{-1}) &=&  1+c_1 z^{-1}+\cdots+c_n z^{-n} 
\end{eqnarray*}
可写为：
$$
\varepsilon(k) = c^{-1}(z^{-1})[a(z^{-1})y(k)-b(z^{-1})u(k)]
$$
\end{frame}

\begin{frame}{二次型指标函数}
将指标函数用二次型近似表示：
\begin{eqnarray*}
J_N &=& \sum_{k=n+1}^{n+N}\varepsilon^2(k)\\
&\approx & (\vect\theta-\hat{\vect\theta}_N)^Tp^{-1}_N(\vect\theta-\hat{\vect\theta}_N)+\beta_N\\
\end{eqnarray*}
\end{frame}

\begin{frame}
利用泰勒级数将$\varepsilon(k)$在估值$\hat\theta$展开：
$$
\varepsilon(k)\approx \varepsilon(k,\hat{\vect\theta})+\left.\left[\frac{\partial \varepsilon(k,\vect\theta)}{\partial\vect\theta}\right]^T\right|_{\hat\theta}(\vect\theta-\hat{\vect\theta})
$$
其中：
\begin{eqnarray*}
\varepsilon(k,\hat{\vect\theta}) &=& e(k) \\
e(k) &=& \hat c^{-1}(z^{-1})[\hat a(z^{-1})y(k)-\hat b(z^{-1})u(k)]
\end{eqnarray*}
\end{frame}

\begin{frame}
可得：
\begin{eqnarray*}
J_{N+1} &=& \sum_{k=n+1}^{n+N+1}\varepsilon^2(k) \\
&\approx & (\vect\theta-\hat{\vect\theta}_N)^Tp^{-1}_N(\vect\theta-\hat{\vect\theta}_N) \\
&&+\beta_N+[e_{N+1}+\vect\psi^T_{N+1}(\vect\theta-\hat{\vect\theta}_N)]^2 
\end{eqnarray*}
其中：
\begin{eqnarray*}
e_{N+1} &=& e(n+N+1) \\
\vect\psi_{N+1} &=& \frac{\partial e_{N+1}}{\partial\hat{\vect\theta}}
\end{eqnarray*}
\end{frame}

\begin{frame}
设：
$$\vect\theta-\hat{\vect\theta}=\Delta$$
得：
\begin{eqnarray*}
J_{N+1}(\vect\theta) &=& \vect\Delta^T(\vect{P}_N^{-1} +\vect\Psi_{N+1}\vect\Psi_{N+1}^T)\vect\Delta \\
&& +2e_{N+1}\vect\Psi^T_{N+1}\vect\Delta+e^2_{N+1}+\beta_N \\
 &=& (\vect\Delta+\vect{r}_{N+1})^T\vect{P}_{N+1}^{-1}(\vect\Delta+\vect{r}_{N+1})+\beta_{N+1}
\end{eqnarray*}
其中：
\begin{eqnarray*}
\vect{P}_{N+1}^{-1} &=& P_N^{-1}+\vect\Psi_{N+1}\vect\Psi_{N+1}^T  \\
\vect{r}_{N+1} &=& \vect{P}_{N+1}\vect\Psi_{N+1}e_{N+1} \\
\beta_{N+1} &=& e_{N+1}^2+\beta_N-e_{N+1}\vect\Psi_{N+1}^T\vect{P}_{N+1}\vect\Psi_{N+1}e_{N+1}
\end{eqnarray*}
$\beta_{N+1}$为已知值，所以
$$
\hat{\vect\theta}_{N+1}=\hat{\vect\theta}_N-\vect{r}_{N+1}
$$
时使$J_{N+1}$最小。
\end{frame}

\bgroup
\setbeamertemplate{sidebar right}{}
\begin{frame}{更新$P_{N+1},\hat\theta_{N+1}$}
利用矩阵求逆引理，得：
\begin{eqnarray*}
\vect{P}_{N+1}^{-1} &=& P_N^{-1}+\vect\Psi_{N+1}\vect\Psi_{N+1}^T  \\
P_{N+1}&=& P_N\left[I-\frac{\Psi_{N+1}\Psi_{N+1}^T P_N}{1+\Psi_{N+1}^T P_N \Psi_{N+1}}\right] \\
r_{N+1}&=&P_{N+1}\Psi_{N+1}e_{N+1} \\
&=&P_N\left[I-\frac{\Psi_{N+1}\Psi_{N+1}^T P_N}{1+\Psi_{N+1}^T P_N\Psi_{N+1}}\right]\Psi_{N+1}e_{N+1} \\
&=&P_N\left[\frac{1+\Psi_{N+1}\Psi_{N+1}^T P_N\Psi_{N+1}-\Psi_{N+1}\Psi_{N+1}^T P_N \Psi_{N+1}}{1+\Psi_{N+1}^T P_N\Psi_{N+1}}\right]e_{N+1} \\
&=&\frac{P_N\Psi_{N+1}e_{N+1}}{1+\Psi_{N+1}^T P_N \Psi_{N+1}} \\
\hat\theta_{N+1}&=&\hat\theta_N-r_{N+1}\\
&=&\hat\theta_N-P_N\Psi_{N+1}(1+\Psi_{N+1}^T P_N \Psi{N+1})^{-1}e_{N+1}
\end{eqnarray*}
\end{frame}
\egroup

\begin{frame}{更新$\Psi_{N+1}$}
$$
\Psi_{N+1}=
\begin{bmatrix}
A  & 0 & 0 \\
0  & B & 0 \\
0  & 0 & C
\end{bmatrix}
\Psi_N+ D
$$
其中：
$$
A=\begin{bmatrix}
-\hat c_1 & \cdots & \cdots & -\hat c_n  \\
1         & \cdots & \cdots &  0          \\
          & \ddots &        &  \vdots     \\
          &        &   1    &   0       
\end{bmatrix}
$$
\end{frame}
\begin{frame}
\begin{eqnarray*}
B &=& \begin{bmatrix}
-\hat c_1 & \cdots & \cdots & -\hat c_n &  0     \\
  1         & \cdots & \cdots &  0      &  0     \\
            & \ddots &        &  \vdots & \vdots \\
            &        &   1    &   0     &  0    
\end{bmatrix} \\
C &=& \begin{bmatrix}
-\hat c_1 & \cdots & \cdots & -\hat c_n \\
  1       & \cdots & \cdots &  0    \\
          & \ddots &        &  \vdots  \\
           &       &   1    &   0   
\end{bmatrix}\\
D &=& [ y_{n+N} ,  0 , \cdots , 0 ,  -u_{n+N+1} , 0 ,  \cdots ,  0 ,-e_{n+N} , 0 , \cdots , 0 ]^T
\end{eqnarray*}
\end{frame}
% $$
% \scriptscriptstyle
% \Psi_{N+1} =
% \left[
% \begin{array}{ccccccccccccc}
% -\hat c_1 & \cdots & \cdots & -\hat c_n & & & & & & & & &  \\
% 1         & \cdots & \cdots &  0        & & & & & & & & &  \\
%           & \ddots &        &  \vdots   & & & & & & & & &  \\
%           &        &   1    &   0       &           &        &        &           &        & & & &  \\
%           &        &        &           & -\hat c_1 & \cdots & \cdots & -\hat c_n &  0     & & & &  \\
%           &        &        &           &   1         & \cdots & \cdots &  0      &  0     & & & &  \\
%           &        &        &           &             & \ddots &        &  \vdots & \vdots & & & &  \\
%           &        &        &           &             &        &   1    &   0     &  0     & & & &   \\
%           &        &        &           &             &        &        &         &        & -\hat c_1 & \cdots & \cdots & -\hat c_n \\
%           &        &        &           &             &        &        &         &        &   1       & \cdots & \cdots &  0    \\
%           &        &        &           &             &        &        &         &        &           & \ddots &        &  \vdots  \\
%           &        &        &           &             &        &        &         &        &            &       &   1    &   0   
% \end{array}
% \right]
% \Psi_N+
% \begin{bmatrix}
% y(n+N) \\
% 0 \\
% \vdots \\
% 0   \\
% -u(n+N+1) \\
% 0 \\
% \vdots \\
% 0  \\
% -e(n+N) \\
% 0 \\
% \vdots \\
% 0
% \end{bmatrix}
% $$

\begin{frame}{$A,B,C$的推导}
从
$$
e(k) = \hat c^{-1}(z^{-1})[\hat a(z^{-1})y(k)-\hat b(z^{-1})u(k)]
$$
得：
\begin{eqnarray*}
\frac{\partial e(k)}{\partial \hat a_i} &=& \hat c^{-1}(z^{-1})y(k-i) \\
\frac{\partial e(k)}{\partial \hat b_i} &=& -\hat c^{-1}(z^{-1})u(k-i) \\
\frac{\partial e(k)}{\partial \hat c_i} &=& -\hat c^{-1}(z^{-1})e(k-i) 
\end{eqnarray*}
进一步有：
\begin{eqnarray*}
\frac{\partial e(k)}{\partial \hat a_i} &=& \frac{\partial e(k-i+j)}{\partial \hat a_j} \\
\frac{\partial e(k)}{\partial \hat b_i} &=& \frac{\partial e(k-i+j)}{\partial \hat b_j} \\
\frac{\partial e(k)}{\partial \hat c_i} &=&  \frac{\partial e(k-i+j)}{\partial \hat c_j} 
\end{eqnarray*}

\end{frame}

\subsection{牛顿-拉卜森递推公式}

\begin{frame}{使用牛顿-拉卜森方法的递推公式：系统差分方程}
$$
a(z^{-1})y(k) = b(z^{-1})u(k)+\frac{1}{d(z^{-1})}\varepsilon(k)
$$
其中：
\begin{eqnarray*}
a(z^{-1}) &=&  1+a_1 z^{-1}+\cdots+a_nz^{-n} \\
b(z^{-1}) &=&  b_0+b_1 z^{-1}+\cdots+b_n z^{-n} \\
d(z^{-1}) &=&  1+d_1 z^{-1}+\cdots+d_n z^{-n} 
\end{eqnarray*}
参数向量为：
\begin{eqnarray*}
\vect{a} &=& [a_1,a_2,\cdots,a_n]^T   \\
\vect{b} &=& [b_o,b_1,\cdots,b_n]^T   \\
\vect{d} &=& [d_1,d_2,\cdots,d_n]^T   \\
\vect\theta &=& [\vect{a}^T,\vect{b}^T,\vect{d}^T]^T
\end{eqnarray*}
\end{frame}

\begin{frame}{计算$\frac{\partial\varepsilon(k)}{\partial\vect\theta}$}
将系统差分方程改写为：
$$
\varepsilon(k) = d(z^{-1})[a(z^{-1})y(k)-b(z^{-1})u(k)]
$$
可得：
\begin{eqnarray*}
\frac{\partial\varepsilon(k)}{\partial a_j} &=& d(z^{-1})y(k-j)=y^F_{k-j},j=1,2,\cdots,n\\
\frac{\partial\varepsilon(k)}{\partial b_j} &=& -d(z^{-1})u(k-j)=u^F_{k-j},j=0,1,2,\cdots,n\\
\frac{\partial\varepsilon(k)}{\partial d_j} &=& a(z^{-1})y(k-j)-b(z^{-1})u(k-j)=-\mu_{k-j},j=1,2,\cdots,n 
\end{eqnarray*}
\end{frame}

\begin{frame}{计算$\frac{\partial\varepsilon(k)}{\partial\vect\theta}$}
$$
\frac{\partial\varepsilon(k)}{\partial\vect\theta} = 
\begin{bmatrix}
\bar{\vect y}^F_{(n)} \\
-\bar{\vect u}^F_{(n+1)} \\
-\bar{\vect \mu}_{(n)}
\end{bmatrix}
$$
其中：
\begin{eqnarray*}
\bar{\vect y}^F_{(n)} &=& [y^F_{k-1}, y^F_{k-2}, \cdots , y^F_{k-n}]^T \\
-\bar{\vect u}^F_{(n+1)} &=& [u^F_k,  u^F_{k-1},\cdots, u^F_{k-n}]^T  \\
-\bar{\vect \mu}_{(n)}  &=& [\mu_{k-1},\mu_{k-2},\cdots, \mu_{k-n}]^T
\end{eqnarray*}
\end{frame}

\begin{frame}{计算$\frac{\partial^2\varepsilon(k)}{\partial\vect\theta^2}$}
$$
\frac{\partial^2\varepsilon(k)}{\partial\theta^2} = 
\begin{bmatrix}
\frac{\partial^2\varepsilon(k)}{\partial\vect{a}^2} & \frac{\partial^2\varepsilon(k)}{\partial\vect{a}\partial\vect{b}} & \frac{\partial^2\varepsilon(k)}{\partial\vect{a}\partial\vect{d}}  \\
\frac{\partial^2\varepsilon(k)}{\partial\vect{b}\partial\vect{a}} &\frac{\partial^2\varepsilon(k)}{\partial\vect{b}^2} &  \frac{\partial^2\varepsilon(k)}{\partial\vect{b}\partial\vect{d}} \\
\frac{\partial^2\varepsilon(k)}{\partial\vect{d}\partial\vect{a}} &  \frac{\partial^2\varepsilon(k)}{\partial\vect{d}\partial\vect{b}} &\frac{\partial^2\varepsilon(k)}{\partial\vect{d}^2} 
\end{bmatrix}
$$
其中：
\begin{eqnarray*}
\frac{\partial^2\varepsilon(k)}{\partial a_j\partial d_m} &=& \frac{\partial^2\varepsilon(k)}{\partial d_m\partial a_j} = y(k-j-m)  \\
\frac{\partial^2\varepsilon(k)}{\partial b_j\partial d_m} &=& \frac{\partial^2\varepsilon(k)}{\partial d_m\partial b_j} = -u(k-j-m)  \\
\end{eqnarray*}
\end{frame}

\begin{frame}{估计准则}
$$
J=\frac{\sum_{k=n+1}^{n+N}e(k)}{2}
$$
梯度：
$$\frac{\partial J}{\partial\hat\theta} = \sum_{k=n+1}^{n+N}e(k)\frac{\partial e(k)}{\partial\hat\theta}=\vect{q}(N)$$
海赛矩阵：
$$
\frac{\partial^2 J}{\partial\hat\theta^2} = \sum_{k=n+1}^{n+N}\left[\frac{\partial e(k)}{\partial\hat\theta}\left(\frac{\partial e(k)}{\partial \hat\theta}\right)^T + e(k)\frac{\partial^2 e(k)}{\partial \hat\theta^2} \right]=\vect{R}(N)
$$
\end{frame}

\begin{frame}{迭代公式}
牛顿-拉卜森公式:
$$
\hat{\vect\theta}_r=\hat{\vect\theta}_{r-1}-R^{-1}(N)q(N)
$$
$q$与$R$的递推公式：
\begin{eqnarray*}
q(N) &=& q(N-1)+e(n+N)\frac{\partial e(n+N)}{\partial\hat\theta}\\
R(N) &=& R(N-1) +\frac{\partial e(n+N)}{\partial\hat\theta}\left(\frac{\partial e(n+N)}{\partial\hat\theta}\right)^T \\
&& +e(n+N)\frac{\partial^2 e(n+N)}{\partial\hat\theta^2} 
\end{eqnarray*}
\end{frame}

% \section{思考}
% \begin{frame}{思考}
% \begin{itemize}
% \item 极大似然法辨识思想
% \end{itemize}
% \end{frame}


%\end{CJK*}
\end{document}
